\chapter{Introduction}

    \section{Introduction}

    % <A brief introduction suitable for a non-specialist, {\em i.e.} without
    % using technical terms or jargon, as far as possible. This may be
    % similar/the same as that in the 'Outline and Plan' document. The
    % remainder of this chapter will normally cover everything to be assessed
    % under the `Background Research` criterion in the mark scheme.>

% TODO: this is a temporary name
\chapter{Lit review}
    \section{The UNIX filesystem}
        \label{sec_UFS}

        For the past 50 years the UNIX style operating system has been one of
        the most prevalent operating system architectures out there. The
        original UNIX introduced a simple and elegant template which is still
        with us to this day. It brough forward many novel ideas: wide hardware
        compatibility, simple and powerful system calls, a powerfull shell,
        minimal design and most importantly a file system \cite{UFS}. All these
        features are still with us, largely unmodified, to this day, although
        greatly improved upon.

        The most important part, the filesystem, had an implementation akin to
        its interface - simple and elegant. It introduced several important
        concepts:

        From a programming view a file is a linear sequence of bytes and can be
        controlled with just 4 system calls - \syscall{open} \syscall{read},
        \syscall{write} and \syscall{close} - without imposing any particular
        structure on them. A file can be \syscall{open}ed by name to get a
        handle to it with which all of its contents can be \syscall{read} and
        \syscall{write} back anything new. The operating system maintains an
        offset showing the current location within the file, so sequential
        calls to \syscall{read} and \syscall{write} continue where the previous
        ones left off. No predefined limits are imposed on file size. To
        enlarge a file programs can just write past the end. A file must be
        \syscall{close}ed after use.

        From an implementation view, every file is assigned a number - its
        i-number. It uniquely identifies this file, regardless of where or how
        many times it appears. It acts as the pointer to the file in the list
        of files, called an i-list (or i-table more recently).

        Each entry in the i-list is an i-node (usually shortened to just
        inode). The inode represents the file and is its header - it cointains
        all metadata with pointers to the actual data.

        To make this flat list of files into a usable tree, special files are
        introduced, called directories, marked by a special bit in the inode.
        The directory is a file whose contents have a special format: a list of
        2-tuples, (name, i-number), which denote that file number
        \textit{i-number} can be found in this directory under name
        \textit{name}. Directories are special in that they cannod be read and
        written to directly. Rather special system calls must be used for the
        operating system to report its contents. The exact method is slightly
        involved and not relevant for this discussion, but it is sufficent to
        say that there are simple library functions roughly equvalent to those
        for normal files.

        File names are alphanumeric strings of any length. To denote that a
        file is contained within a directory, a single forward slash ("/") is
        used, meaning whatever is on its right is contained on the directory
        named on the left. This can be repeated an unlimited number of times.
        The root directory has no name \textit{per se}, hence any path refering
        to it starts with a slash, for example \textit{/file/in/root} is a file
        called \textit{root} in a directoy \textit{it} itself contained in
        directoy \textit{file} which is in the root of the filesystem.

        File number 1 is defined to be the the root directory (/) to bootrstrap
        the search and inode 0 is definted to not exist.

        An important, but not user visible detail is that the filesystem has a
        list of free blocks (called the free list), to denote which blocks are
        unused by the filesystem. Here, a block is a fixed chunk of space on
        the drive set on creation of the filesystem. It is different, but
        closely related to, a disk sector which is the smallest unit a drive
        can operate on.

    \section{Evolution of implementation}

        \subsection{UFS}

            The first implementation of this scheme appeared in UNIX, dubbed
            UFS (short for the UNIX filesystem), was very simple.  It used a
            linear and fixed size i-list. An inode had 13 pointers to blocks.
            The first 10 point directly to data, while the next 3 contain lists
            of blocks. They have respectively 1, 2 and 3 levels of indirection
            to reach a data block. The free list was a simple linked list,
            where each block had pointers to free blocks and a pointer to the
            next block in the list \cite{UNIX_implementation},
            \cite{UNIX_source_commentary}.

            Over time, however, it turned out that this arrangement was very
            inefficient. In particular, data blocks for a file lacked locality,
            as did the i-numbers for files in the same directory. Additionally,
            the free list would become very scrambled in that logically
            sequential entries may be physically very far apart on the disk.
            These problems got exacerbated after prologned use. On top of that,
            hard drives (the main storage medium at the time) had physical
            spinning platters with a head that had to move across and wait for
            the block to rotate under it in an operation called a "seek". The
            seek time between arbitrary blocks was large, a quirk present on
            hard drives to this day. As a result, throughput could be as low as
            4\% \cite{FFS}.

        \subsection{FFS}
            \label{sec_FFS}

            \citeauthor{FFS} proposed many improvements to increase
            performance. For a start, block size was increased 8 fold from 512
            bytes to 4096 (or even 8192). This improves locality and
            performance pretty much for free. This is because a larger block
            allows the hard drive to read for longer before a seek might be
            required. A side effect is that the data fits in fewer blocks which
            reduces the indirection to index them and as a result the need for
            seeks. Extra complexity was introduced to minimise wasted space
            from small files. Further, blocks were grouped together into
            cylinder groups, where a cylinder is a physical feature of a hard
            drive, blocks on which have very good sequential performance
            (accessing the file in sequence). Then, policies were introduced to
            put the inodes of files in the same directory in the same cylinder
            group as well as their data blocks to reduce seeking. Finally, the
            free list became a bitmap of a fixed size, eliminating the
            scrambling of logically sequential entries that further degrade
            performance.

            The result was that \citeauthor{FFS} measured an increase in disk
            bandwidth utilisation of about an order of magnitude without an
            increase in wasted space.

            An important note is that cylinder groups include redundant copies
            of various metadata to increase resiliance to data loss from damage
            to the disk. This shows that reliability came into the picture very
            early in the history of UNIX like filesystems and the issue of data
            recovery was given active thought to.

        \subsection{LFS}
            \label{sec_LFS}

            With advances in software and hardware, by \citeyear{IO_bottleneck}
            file access and reliability patterns became apparent. Especially
            for office use, small reads and writes dominated the workload. Like
            in UFS and FFS, the mechanical nature of hard drives made such
            operations costly when performed in a random manner. To imporove
            performance, caching is identified as a substantial improvement,
            which has been wildly used since \cite{IO_bottleneck}
            \cite{Linux_VFS_cache}.

            However, \citeauthor{IO_bottleneck} points out some issues with
            such an approach: a system crash or a power outage puts the
            contents of such a cache in jeopardy. This is not a problem for a
            read cache, but a write cache loss can be catastraphic. This brings
            reliability into the picture and balancing it with performance
            becomes a key factor.

            Further, \citeauthor{IO_bottleneck} argues that such caching shifts
            the IO pattern from mostly random reads to mostly writes in
            occasional big bursts of relatively large amount of data.
            Maintaining the popular layout of filesystems so far does not make
            use of this fact, so the author suggests structuring the filesystem
            as a log - an append-only structure which wraps around once it
            fills up. This gives locality to files that are used close
            together, utilises the burst nature of writes and recovery is easy
            - only the end may get corrupted \cite{IO_bottleneck}.

            \citeauthor{LFS}, reiterates the necessity of asynchronous writes
            (and therefore a cache). In his LFS, based on \cite{IO_bottleneck},
            the inode structure is kept exactly the same as in FFS (including
            the data block arrangement), but its layout policy was redically
            different. First, the i-list became a regular file, but instead of
            holding inodes, it held addresses to where they could be found. The
            inode itself and its data are laid our sequentially elsewhere to
            improve performance and the free list is done away with. Instead,
            the disk is split into large continuous chunks of free space -
            \textbf{extents} (analogous to segments in LFS) - which contain a
            single block of metadata that includes that information.  Special
            care is taken to maintain these chunks large. This is done to
            reduce fragmentation and to allow a better likelyhood for
            sequential accesses. This idea for large extents remains with us
            today.

            New data is written in a log fashion. There are several fixed
            places where a pointer to the i-list is stored and there is a
            single fixed place that is dedicated to storing the last known good
            head of the log to ensure reliability.

        \subsection{WAFL}
            \label{sec_WAFL}

            With time reliability became an ever growing concern, both from a
            techincal and from a human perspective. The idea of snapshots
            contributes to both of these.

            Snapshots are read-only copies of the entire file system. WAFL
            \cite{WAFL} keeps many of them, allowing for old versions of files
            to be viewed after deletion. To achieve this it uses copy-on-write
            (CoW) semantics for disk blocks. This means blocks have to be
            copied elsewhere to write to them. Snapshots are essentially
            pointers to blocks. CoW allows a new snapshot to consume extra
            space only for the difference with another snapshot. Block are
            freeded after all references to them have been removed. This has
            the added benefit that it avoids duplicating data to store the
            snapshots.

            Besides file versioning, snapshots are useful for backups on a live
            system, as creating them is pretty much much free (see below).

            The way WAFL implemnts the CoW is by creating a tree of blocks,
            where the root is a known fixed block. Then it has all metadata
            (like the free list) be a file too. The inodes of all of these
            files are stored in the inode list, itself a file, whose inode
            itself is stored in the root block. This way every single bit of
            information required for the filesystem's operation is connected in
            a rooted tree and copying the root block makes a snapshot.
            Modifying any other block requires copying it to a new location
            with the modificaiton only on the new copy and modifying the
            reference to it. Modifying the reference requires the same
            operation, which eventually bubbles up to the root block which can
            be modified in place.

            The benefit of this approach is that the filesystem is always
            consistent. At no point is any data modified in place, meaning a
            crash cannot cause corruption. Rather a new copy is built somewhere
            in free space and only "added" to the filesystem when the whole
            branch of the tree is complete. Writing a single block is assumed
            to be atomic \cite{drive_atomicity} so updating the root is an all
            or nothing operation, limiting the impact of a crash to only some
            data loss that hasn't had the time to be commited to disk.

    \section{The B-tree}
        \label{sec_btree}

        B-trees are a de facto standard for file organisation
        \cite{btree_ubiquitous}. Most modern filesystems incorporate them in
        some way, like ext4 (section \ref{sec_ext4}), XFS
        \cite{XFS_scalability}, HFS+ \cite{HFSplus} and, probably most notably,
        BTRFS (short for "B-tree File System") \cite{BTRFS}. They are very well
        suited to secondary storage based applications (like filesystem) due to
        their large node sizes and relatively shallow depths which fit quite
        neatly into their access patterns.

        % TODO: prime candidate for graphics
        As explained by \citeauthor{btree_ubiquitous}, B-trees are a
        generalisation of binary search trees. Each node can contain any fixed
        maximum number of keys (instead of the one for a binary tree) and the
        same number plus one children. Keys are integers and are kept sorted
        within a node. Children are assigned to parents such that all of the
        keys in a child node lie between two adjacent keys in the parent (or
        just one in case it is located at either end). Nodes do not have to be
        full, although non-leaves are kept at least $1/2$ full. When a node
        fills up it is split into two equal parts plus the middle key. The
        middle key is inserted into the parent to act as a separator while the
        two halves are attached on either side. If the parent fills up too the
        process can repeat up to the root. If a node is too empty, its keys are
        redistributed to adjacent nodes. This prevents a sparse tree from
        forming which would increase its size.

        Typically, the capacity of a node is chosen such that it aligns with a
        single block on disk. This way a node can be transfered to and from
        memory in a single operation. Since blocks are large, nodes contain
        lots of keys so the tree ends up being rather shallow. In fact, if the
        tree is maintaned balanced, it always has a depth logarithmically (of
        base the maximum number of keys in a node) proportional to the number
        of keys in the tree. This gives the shallowness property. Additionally,
        this means that relatively few accesses are needed to reach a leaf node
        which tend to be random.

        Nodes can be efficiently searched when loaded to memory (with a binary
        search or otherwise). Secondary storage tends to be slow in comparison
        and accessing it dominates the time spent. As a result, a typical
        metric for measuring B-tree operation costs in the context of secondary
        storage is the number of blocks accessed \cite{btree_ubiquitous}. Then
        all operations require a logarithmic number of node accesses in terms
        of the size of the tree (times a different constant depending on the
        operation).

        A particular variant, the \bplustree only stores keys in leaf nodes and
        inner nodes are only used for path finding. Adjacent leaf nodes are
        also linked together to enable constant time sequential access, as is
        common with files.

    \section{ext4}
        \label{sec_ext4}

        The most modern version of the ext filesystem, ext4, is native and
        usually the default on Linux. It was build by the Linux community
        specifically for it. Since Linux is a UNIX (\ref{sec_UFS}) derivative,
        so is the ext family. As a result, it integrates the principles and
        improvements over the past 50 years while maintaining the same general
        idea.

        Ext4 is a gold standard for modern filesystems as it is the default on
        many (if not most) major Linux distributions like Debian
        \cite{Debian_filesystem} and Red Hat \cite{RedHat_filesystem}. It is
        actively maintained \cite{ext4_mailing_list}, with the latest Linux
        release (5.17) adding 74 commits to it \footnote{\monospace{git log
        --numstat v5.16..v5.17 fs/ext4})}. It has existed since 2006
        \cite{ext4_origin} and it is about \numprint{60000} lines of code
        \footnote{\monospace{cloc fs/ext4}} so it is very mature. Finally,
        unlike its most direct competitiors by the likes of Windows' NTFS and
        MacOS' APFS, it is free and open source, so code and documentation are
        readily available.

        \subsection{Use of B-trees}
            \label{sec_htree}

            Ext4 uses B-trees in two places - extent trees \cite{ext4_docs} and
            directory indexing. An extent tree is a \bplustree in which files
            store the list of extents that make up its data. It is a somewhat
            modified implementation, most notably lacking references between
            leaf nodes, and the documentation does not refer to is as such.

            Directory indices use a heavily modified version of a B-tree known
            as an HTree (meaning Hash tree) \cite{HTree}. The major difference
            is that keys in the tree are hashes of directory entry names
            instead of the names themselves. This way the tree can implement
            ranges of hashes (like buckets in a hash table) in its leaves. This
            solves the problem of rehashing (rebalancing) a hash table, by
            allowing incremental changes. Additionally, leaves are kept at a
            uniform depth to remove the impact of rebalancing the tree and
            maintaining good performance.

        \subsection{Data locality}
            \label{sec_locality}

            Ext4 employs a range of tricks to maximise locality of various
            data. This has the obvious benefit of reducing seek times on
            spinning hard drives.

            First, it uses blocks of at least 4KB, like FFS (\ref{sec_FFS}),
            but larger (powers of 2) are supported too. Allocations on file
            creation are speculative and large in size to give space for the
            file to grow in a contiguous chunk of space. Additionally,
            allocation is delayed as long as possible, to ensure unwritten
            chunks can be coalesced into larger ones and laid out better. It
            also tries to keep the inode close to both its data and its parent
            directoy on the presumption that they are related and accessing
            them in sequence is likely.

            A second trick is that the filesystem is split into large block
            groups in which the above rules are applied. A major exception is
            that root directoy entries are spread out as much as possible to
            encourage them to grow independent and reduce fragmentation.

            The final interesting technique is that ext4 uses extents which are
            a "range of contiguous physical blocks owned by a file"
            \cite{ext4_space_maps}. Using them allows to reduce the amount of
            necessary metadata for those large contiguous chunks within a block
            group which imporoves performance, like in LFS (\ref{sec_LFS}).

        \subsection{Metadata integrity}

            System crashes are a consistent issue. Causes include dormant bugs
            in the kernel, applications misbehvaing at critical moements or a
            mundane improper shutdown. Addtionally, many machines lack the
            hardware capabilities to detect or correct memoery errors, as Linus
            Torvalds notes \cite{Linus_ECC_rant}. As a result, even the best
            code can be brought down by completely unforseeable events like bit
            flips due to a stray gamma rays.

            To that effect, system crashes are assumed to be inevitable and
            steps are taken to address them. This is especially true for
            filesystems as data loss is one of the most critical, annoying and
            time consuming failures possible. There are various ways to
            minimise the chances of that happening but two major ones are Soft
            updates, where metadata is only written in a very specific order to
            ensure consistency \cite{soft_updates} and journaling where
            metadata is written to a small LFS style log before entering the
            filesystem proper \cite{ext4_docs}. Pros and cons differ and
            \citeauthor{journaling_vs_soft_updates} presents a thorough
            comparison. The ext familiy settled on journaling.

            Another reliability feature is that all ext4 data structures are
            checksummed wherever possible (exceptions are due to backwards
            compatibility). This greatly helps with combating silent
            filesystem corruption and can help the kernel limit damage.

            It is important to note that despite these reliability features,
            only metadata enjoys their guarantees. Data itself (like file
            contents) have no checks whatsoever. It is possible to enable
            journaling for data, however, the performance impact is severe
            \cite{ext4_docs}. Adding other guarantees for it requires backwards
            incompatible changes to the filesystem, which the Linux kernel is
            famously unfavourable towards with the "Never break userspace"
            motto \cite{never_break_userspace}.

    \section{Data integrity}

        \subsection{drive reliability}
            \label{sec_reliability}

            Drives are unreliable \cite{RAID}, \cite{Backblaze_stats}. Due to
            their mechanical nature they wear out and stop working. They do
            this at an unpredictable rate for individual drives, but at
            relatively constant rates over large arrays \cite{Backblaze_stats}.
            Tools for monitoring drive health, like SMART, have been developed
            to combat the issue and are in general a good indicator for the
            likelyhood of loss of the drive.

            However, drive failure is not binary. They do not work flawlessly
            until such a time as to not work at all. Individual sectors develop
            errors over time, failing to be read or written to.  Drives try to
            combat the issue by keeping an array of spare sectors that replace
            misbehvaing ones when detected. To determine if a sector is
            misbehvaing in the first place, drives employ ECC codes
            \cite{data_corruption_storage_stack}.

            However, despite all the best efforts of drives and their
            manufacturers, drives still cannot be relied upon to report correct
            data all of the time \cite{data_corruption_storage_stack}.
            Occasionally, they will report an unrecoverable error to the
            operating system. Even more rarely, drives will return incorrect
            data without indicating errors. As a consequence, individual drives
            are never relied upon for the data to be both present and intact.
            Other techniques are always employed to ensure both are
            independently maintained. Otherwise data loss is always at risk.

            SSDs and flash storage in general are a similar story. As
            \citeauthor{2D_RAID} puts it: "Flash is basically garbage. It
            barely works. We tolerate it because it is fast". SSDs employ
            similar techniques as spinning hard drives and suffer similar
            problems as them \cite{flash_large_scale},
            \cite{flash_reliability}. Their failure patterns are substantially
            different but the end result is the same - unrecoverable errors
            that would lead to data loss if not accounted for outside the
            drive.

            One way to measure these failures is with the raw bit error rate
            (RBER). The RBER is defined as the number of currupted bits per the
            number of total bits read \cite{flash_reliability}.
            \citeauthor{flash_reliability} takes measurements over a vast
            number of drives with single-level cell (SLC, 1 bit per memory
            cell) and multi-level cell (MLC, 2 bits) chips. They measure RBERs
            between $10^{-8}$ and $10^{-5}$. \citeauthor{bit_error_mlc} finds
            similar rates for MLC. On the other hand \citeauthor{bit_error_qlc}
            finds that triple-level cell (TLC) and quad-level cell (QLC) flash
            suffer RBER rates of around $10^{-3}$ and $10^{-2}$ respectively.
            Although \citeauthor{flash_reliability} also notes that RBER rates
            are a poor indicator of unrecoverable errors it puts an upper bound
            on their likelyhood.

            As a result we can expect a worst case failure rate of a few bits
            per kilobyte of data read. As \citeauthor{flash_reliability} and
            \citeauthor{flash_large_scale} note, these failures are not uniform
            and change with SSD age and wear. With further analysis from
            \citeauthor{flash_error_manual} it is apparent that this is due to
            a wide range of physical phenomena. However, it becomes apparent
            that these errors are relatively isolated from each other where
            only adjacent cells can affect each other due to the way cells
            store electrons.

            Further, due to effective error correcting techniques
            \cite{flash_reliability} \cite{bit_error_mlc} \cite{bit_error_qlc}
            \cite{flash_error_manual} actual unrecoverable errors are much
            lower than the RBER. Therefore, we can expect any higher level
            recovery mechanisms to be needed much less frequently than might be
            expected.

        \subsection{RAID}
            \label{sec_RAID}

            Non-volatile storage has historically been orders of magnitude
            slower than volatile memory, like RAM, which is itslef much slower
            than CPUs and their caches \cite{IO_bottleneck}. Disk arrays are an
            attempt to increase IO performance and bridge that gap. Since
            drives are inherently unreliable (\ref{sec_reliability}), putting
            many of them together exacerbates the problem. RAID (a redundant
            array of inexpensive disks) is one solution for the problem of
            drive reliability \cite{RAID}. As its name suggests, RIAD adds
            redundancy to disk arrays in different configurations. Popular
            arrangements are RAID levels 1, 4 and 5.

            RAID 1 is two drives acting as one, effectively a mirror. If one of
            them fails the other is a perfect copy.

            RAIDs 4 and 5 are very similar. Both calculate extra parity for
            each block (a simple method is XORing the respective blocks in
            three disk arrays, but more advanced schemes like Hamming codes are
            also used) and store it on an extra disk. The major difference is
            lay out of parity across the disks. The result is that the array
            can experience the loss of any of the disks while keeping all of
            the data intact. It has a benefit over RAID 1 in that it is
            cheaper and more space efficient.

            Since the original paper, other RAID variants have been developed
            like RAID 6 \cite{RAID6} - RAID 5 with an extra parity drive

        \subsection{At scale}

            A unique case for data reliability are datacenters for which data
            loss is not an option. They also operate at massive scales, where
            conventional approaches break down or become severe bottlenecks.
            Hence many, if not most, of these companies have special bespoke
            arrangements to fulfill their needs.

            Conveniently, Backblaze, a cloud storage provider, has an open
            approach to its infrastructure, frequently publishing in-depth
            information and open sourcing parts of their infrastructure.

            Their solution to reliability \cite{Backblaze_arch} is conceptually
            a distributed RAID 5 over 20 drives, 3 of which are parity.
            However, they do not use conventional RAID.  Instead, they use
            custom software to split files into chunks, called "shards", which
            are then stored on normal ext4 filesystems with parity calculated
            with a Reed-Solomon erasure code. The desgin is motivated by the
            good reliability and performance of ext4 and that inverting the
            usual order of replication and filesystem allows them to protect
            against filesystem corruption as well.

            Additionally, every shard is checksummed to protect against data
            corruption. This is necessary because "once in a while they return
            the wrong data, or are just unable to read a given sector"
            \cite{Backblaze_arch} and software needs to be able to identify the
            error and recompute the data from the parity.

        \subsection{Caching}

            Caching is an integral part of modern operating systems
            \cite{IO_bottleneck} \cite{LFS} \cite{LFS} \cite{FFS}. All modern
            variants employ it in various very visible ways. Reads are
            routinely cached and so are writes although with extra care. This
            practice is so prevalent that opening files/programs the first time
            is frequently orders of magnitude slower than any subsequent
            access. In particular, it is essential in obtaining good, almost
            sequential, performance out of random small writes
            \cite{soft_updates} \cite{LFS}.

            Caching, however, is tricky especially on the write side. Besides
            the usual cache coherence issues, caching of writes for prologned
            periods of time introduces a risk of data loss. To avoid this
            possibility while not diminishing the effectiveness of techniques
            like soft updates \cite{soft_updates} or log based filesystem
            (\ref{sec_LFS}), operating systems have to perform a fine balancing
            act.

            Linux in particular caches aggresively \cite{Linux_VFS_cache}.
            Luckily, its VFS layer implements this in an abstract manner, so
            individual filesystems do not need to implement it themselves.
            Nevertheless, they frequently need to be aware of it and in rare
            cases, like ZFS (\ref{sec_ZFS}), they need special behaviour to
            implement some features of the filesystem.

        \subsection{ZFS}
            \label{sec_ZFS}

            % Even a filesystem like ZFS with incredible reliability
            % characteristics \cite{ZFS_reliability} is vulnerable to this (referes scrubbing)

            Considered by many to be the most robust enterprise ready
            filesystem (like TrueNAS does \cite{TrueNAS_enterprise}), ZFS takes
            all necessary aspects of a filesystem and tries to combine them
            into one as close to bulletproof solution. It is widely recommended
            in place of almost any other solution outside large scale
            enterprise world. It integrates a very wide variety of features to
            ensure data integrity while maintaining performance.

            In ZFS all blocks are checksummed \cite{ZFS}. This checksum is
            stored on any other block that saves a pointer to it. This way data
            and error correction metadata are separated and the likelyhood of
            simultaneous corruption of both is reduced. Blocks are chained in a
            tree. The root of the tree is called the uberblock. Leaves of the
            tree contain data (inodes, their data, directories, etc). The
            uberblock stores its own checksum but is the only one to do so. To
            mitigate against its corruption, several redundant copies of it are
            stored.

            Modifications to the tree of blocks is done in the same
            copy-on-write (CoW) manner as in WAFL (\ref{sec_WAFL}) so it is
            always consistent. The only critical operation is the uberblock
            overwrite, but its checksum allows for any failure to be detected
            and a redundant copy read.

            To pave the way for efficient multidrive arrangements, ZFS acts as
            a volume manager and implements pools. A pool is an array of vdevs
            (virtual devices) which provide storage. Their space is
            concatenated together and handed out to every different volume
            (what ZFS calls an individual filesystem) on a per-reqest basis.

            Vdevs exist to provide redundancy. A vdev is a node with 1 or more
            children. Children can be either drives or other vdevs. Vdevs come
            in different types to implement various features: for example a
            mirroring vdev mirrors its accesses to both children. Alternatively
            a vdev can implement RAID 5, concatenation or any other useful
            operation. Since children can be either drives or other vdevs,
            vdevs allow for drive trees of arbitrary complexity and depth.
            Since pools concatenate vdevs, different drives can be grouped
            together and different groups can have different redundancy
            configurations, drive sizes and types depending on their
            characteristics.

            Even though ZFS also goes to great lengths to allocate data blocks
            in contiguous chunks of space, ZFS reads and writes tend to be
            slower than those of other filesystem even when paired with RAID
            functions due to the CoW "bubbling" mechanism having to overwrite
            many blocks \cite{ZFS} (\ref{sec_WAFL}). To counteract this ZFS
            caches extensively. So much so that TrueNAS (a popular home file
            server operating system) has a dedicated guide on how much memory a
            server should have \cite{TrueNAS_hardware_guide} and more memory is
            frequently recommended on forums. The caching is such an important
            part of ZFS that it does not rely on the operating system and is
            instead implemented separately.

            ZFS caching has 2 important parts: The ARC and the ZIL. The ARC
            (Adaptive Replacement Cache) is a cache that dynamically adjusts
            its caching policy to current usage patterns to store items that
            other caches may fail to identify \cite{ARC}. It also allows a
            second level of caching (the L2ARC) on high speed storage for data
            that is not top caching priority but is accessed frequently enough
            to be kept around.

            The copy-on-write nature of the filesystem makes it more likely for
            contiguous chunks of data to be scattered around the disk. Then the
            tree organisation requires many indirect (and far apart) accesses
            to verify the integrity of a block. This combination greatly
            degrades performance, necessitating the use of the ARC. Meanwhile,
            to ensure that the same guarantees as data on disk are extended to
            the contents of the ARC, use ECC memory is strongly recommended too
            \cite{TrueNAS_hardware_guide}.

            The ZFS Intent Log (ZIL) acts like and solves similar problems to
            LFS (\ref{sec_LFS}). ZFS is always consistent on disk but because
            of copy-on-write, performing these updates frequently requires many
            bubbling operations up to the uberblock, degrading performance. The
            ZIL acts as a stopgap for writes (of both data and metadata) that
            have been accepted, cached and not flushed to disk yet. It is a
            log, similar to LFS, that records all writes in sequence until they
            can be commited on disk. In the event of system failure uncommited
            writes can be recovered.

            In summary, ZFS tries to account for \textit{every} eventuality and
            makes arrangements to mitigate them. In places where this affects
            performance, further steps are taken to minimise their impact, such
            as various caching mechanisms. The result is a watertight design
            which is infamous for reliability across the industry.

\chapter{Problem setting}

    As we have seen, non-volatile storage devices are very error-prone and not
    particularly reliable (\ref{sec_reliability}). To account for this, modern
    filesystems, like ext4 (\ref{sec_ext4}), perform metadata checksumming to
    avoid filesystem corruption. However, this still leaves actual data in the
    filesystem vulnerable. There exist various methods to combat the issue,
    like disk based parity RAID (\ref{sec_RAID}) or dedicated filesystems like
    ZFS (\ref{sec_ZFS}). However, these solutions are not perfect and suffer
    from some unfortunate flaws. There are notable examples of data loss even
    when using them \cite{LTT_data_loss}. This section examines these
    shortcomings and presents the problem this dissertation will tackle.

    \section{The write hole}
        \label{sec_RAID_problems}

        RAID 5 has a critical issue: writing data and experiencing a system
        crash can silently corrupt unrelated data on the disk. This phenomenon
        is known as the write hole \cite{LWN_md_journal}.

        The issue is in the fact that writing a single block (or stripe) of
        data on a RAID 5 array requires at least two writes to two independent
        drives. The first is the actual block where the data is stored and the
        second is its parity computed from it and all asociated blocks on other
        drives. If the system were to experience any kind of failure in between
        these two writes then the array is left in an inconsistent state. The
        data on the non-parity disks will be correct but their parity will not
        (or vice versa but the problem is the same regardless of order).
        However, when the system detects the issue it cannot repair it
        correctly. When parity is recalculated it will not match the one on
        disk, which is an older version. Alternatively, the system can
        recompute any of the associated blocks it might believe to be the
        issue. That can either be the block written just before the failure,
        resulting in reverting the write, or any of the related blocks,
        resulting in overwriting correct data. Even worse, if any drive fails
        before the array can be rapaired, then one of these will happen
        silently, depending on which drive fialed.

        The problem lies in the fact that the system cannot distinguish between
        these three options and the second one leads to data loss. The parity
        calculations will be valid in any of the three configurations. The only
        difference is which block we select as incorrect and the hardware
        cannot give us a hint as all of them will be functional from its
        perspective. What is more, the data loss can be completely unrelated to
        the file, so a filesystem journal cannot hope to correct the issue.

        % RAID-z fixes this. i need to argue why it's bad
        % insert reliability analysis of zfs in the zfs section?
        % https://web.archive.org/web/20141216015058/https://blogs.oracle.com/bonwick/en_US/entry/raid_z
        % TODO:
        % Explain the extreme case?: a RIAD 1 mirror. If one is (silently) wrong,
        % which one can be trusted? Neither.

        To "close" the write hole, Linux implements two features: a journal for
        RAID \cite{LWN_md_journal} (also called a cache occasionally) and a
        partial parity log (PPL) \cite{partial_parity_log}. The journal uses a
        separate drive to store a journal of writes, much like the ZIL, ext4
        and LFS (\ref{sec_ext4}, \ref{sec_ZFS} and \ref{sec_LFS}) and then
        recover any uncommited changes on startup. Alternatively, the PPL
        stores extra (partial) parity in the metadata area of parity drives,
        again allowing for recovery on startup.

        Both of these are imperfect solutions though. The journal requires an
        additional drive (however small) which is very cumbersome for small
        systems like laptops (although new technologies like Intel Optane
        \cite{Intel_Optane} reduces the concern but it is expensive and slots
        to plug it in are not in abundance yet). Regardless, the journal is
        susceptible to the same issues as the drives it is trying to protect.
        Therefore inevitable errors in the log drive (\ref{sec_reliability})
        can diminish its benefit. On the other hand, the PPL's  documentation
        \cite{partial_parity_log} concedes it reduces performance by up to 40\%
        and in the event of a loss of a (dirty) drive its behaviour is the same
        as unaltered RAID.

    \section{ZFS resources}

        The reliability of ZFS comes at a very steep price - system resources
        \ref{sec_ZFS}. Due to its caching policy, main memory requirements are
        very high and not satisfying them can and will degrade performance.
        Workstation class devices are not dedicated file servers and rely on
        their memory for other tasks - 3D morelling, rendering, compillation
        and so on. Additionally, increasing available memory can be impossible
        for laptops. Conversely, these same tasks need the speed and
        reliability offered by ZFS for doing work effectively, as data loss can
        be catastrophic.

        Alternatively, for small home servers the extra cost for memory may be
        unjustifiable for a small system with a low-end CPU and a hadnful of
        drives. Features like the ZIL may not be incorporated too, making ZFS
        performance unjustifiable.

        All of this is ignoring the fact that ZFS tends to run best on
        dedicated or more specialist operating systems like TrueNAS or Linux
        which may be incompatible with home Windows based machines or less
        techincally savvy users.

    \section{The problem to address (conclusion)}
        \label{sec_problem}

        Neither mechanical nor solid state drives cannot be relied upon. It is
        evident that there exist a plethora of techniques to ensure data
        integrity on a wide range of operating systems despite these
        shortcomings. However, there exists a reliability gap for small
        personal computers, workstations and home servers. Filesystems that are
        intended for them, like ext4, do not provide sufficent data integrity
        guarantees. Those that do can, like ZFS, be infeasible due to
        financial, spacial or complexity concerns. Simple solutions, like RAID,
        have significant flaws by default and addressing them presents similar
        challenges as to ZFS. Therefore a minimal filesystem with ext4-like
        features and basic amounts of redundancy to provide data guarantees at
        small scales is missing while there could be significant benefits from
        using one.
