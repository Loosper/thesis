% Sections and sub-sections can be declared using \section and \subsection.
% There is also a \subsubsection, but consider carefully if you really need
% so many layers of section structure.
\chapter{Introduction and Background Research}

    \section{Introduction}

    % <A brief introduction suitable for a non-specialist, {\em i.e.} without
    % using technical terms or jargon, as far as possible. This may be
    % similar/the same as that in the 'Outline and Plan' document. The
    % remainder of this chapter will normally cover everything to be assessed
    % under the `Background Research` criterion in the mark scheme.>

% TODO: this is a temporary name
\chapter{Lit review}
    \section{The UNIX filesystem}
        \label{sec_UFS}

        For the past 50 years the UNIX style operating system has been one of
        the most prevalent operating system architectures out there. The
        original UNIX introduced a simple and elegant template which is still
        with us to this day. It brough forward many novel ideas: wide hardware
        compatibility, simple and powerful system calls, a powerfull shell,
        minimal design and most importantly a file system \cite{UFS}. All these
        features are still with us, largely unmodified, to this day, although
        greatly improved upon.

        The most important part, the filesystem, had an implementation akin to
        its interface - simple and elegant. It introduced several important
        concepts:

        From a programming view a file is a linear sequence of bytes and can be
        controlled with just 4 system calls - \syscall{open} \syscall{read},
        \syscall{write} and \syscall{close} - without imposing any particular
        structure on them. A file can be \syscall{open}ed by name to get a
        handle to it with which all of its contents can be \syscall{read} and
        \syscall{write} back anything new. The operating system maintains an
        offset showing the current location within the file, so sequential
        calls to \syscall{read} and \syscall{write} continue where the previous
        ones left off. No predefined limits are imposed on file size. To
        enlarge a file programs can just write past the end. A file must be
        \syscall{close}ed after use.

        From an implementation view, every file is assigned a number - its
        i-number. It uniquely identifies this file, regardless of where or how
        many times it appears. It acts as the pointer to the file in the list
        of files, called an i-list (or i-table more recently).

        Each entry in the i-list is an i-node (usually shortened to just
        inode). The inode represents the file and is its header - it cointains
        all metadata with pointers to the actual data.

        To make this flat list of files into a usable tree, special files are
        introduced, called directories, marked by a special bit in the inode.
        The directory is a file whose contents have a special format: a list of
        2-tuples, (name, i-number), which denote that file number
        \textit{i-number} can be found in this directory under name
        \textit{name}. Directories are special in that they cannod be read and
        written to directly. Rather special system calls must be used for the
        operating system to report its contents. The exact method is slightly
        involved and not relevant for this discussion, but it is sufficent to
        say that there are simple library functions roughly equvalent to those
        for normal files.

        File names are alphanumeric strings of any length. To denote that a
        file is contained within a directory, a single forward slash ("/") is
        used, meaning whatever is on its right is contained on the directory
        named on the left. This can be repeated an unlimited number of times.
        The root directory has no name \textit{per se}, hence any path refering
        to it starts with a slash, for example \textit{/file/in/root} is a file
        called \textit{root} in a directoy \textit{it} itself contained in
        directoy \textit{file} which is in the root of the filesystem.

        File number 1 is defined to be the the root directory (/) to bootrstrap
        the search and inode 0 is definted to not exist.

        An important, but not user visible detail is that the filesystem has a
        list of free blocks (called the free list), to denote which blocks are
        unused by the filesystem. Here, a block is a fixed chunk of space on
        the drive set on creation of the filesystem. It is different, but
        closely related to, a disk sector which is the smallest unit a drive
        can operate on.

    \section{Evolution of implementation}

        \subsection{UFS}

            The first implementation of this scheme appeared in UNIX, dubbed
            UFS (short for the UNIX filesystem), was very simple.  It used a
            linear and fixed size i-list. An inode had 13 pointers to blocks.
            The first 10 point directly to data, while the next 3 contain lists
            of blocks. They have respectively 1, 2 and 3 levels of indirection
            to reach a data block. The free list was a simple linked list,
            where each block had pointers to free blocks and a pointer to the
            next block in the list \cite{UNIX_implementation},
            \cite{UNIX_source_commentary}.

            Over time, however, it turned out that this arrangement was very
            inefficient. In particular, data blocks for a file lacked locality,
            as did the i-numbers for files in the same directory. Additionally,
            the free list would become very scrambled in that logically
            sequential entries may be physically very far apart on the disk.
            These problems got exacerbated after prologned use. On top of that,
            hard drives (the main storage medium at the time) had physical
            spinning platters with a head that had to move across and wait for
            the block to rotate under it in an operation called a "seek". The
            seek time between arbitrary blocks was large, a quirk present on
            hard drives to this day. As a result, throughput could be as low as
            4\% \cite{FFS}.

        \subsection{FFS}
            \label{sec_FFS}

            % maybe mention that increasing block size decreases the need for
            % indirection
            \citeauthor{FFS} proposed many improvements to increase
            performance.  For a start, block size was increased 8 fold from 512
            bytes to 4096 (or even 8192), which improved locality and reduced
            seek times, essentially for free, although extra complexity was
            introduced to minimise wasted space from small files. Further,
            blocks were grouped together into cylinder groups, where a cylinder
            is a physical feature of a hard drive, blocks on which have very
            good sequential performance (accessing the file in sequence). Then,
            policies were introduced to put the inodes of files in the same
            directory in the same cylinder group as well as their data blocks
            to reduce seeking. Finally, the free list became a bitmap of a
            fixed size, eliminating the scrambling of logically sequential
            entries that further degrade performance.

            The result was that \citeauthor{FFS} measured an increase in disk
            bandwidth utilisation of about an order of magnitude without an
            increase in wasted space.

            % TODO: do i need this
            % An important note is that cylinder groups include redundant copies
            % of various metadata to increase resiliance to data loss from damage
            % to the disk.

            % Another important note is that filesystems which are near their
            % capacity suffer greatly decreased performance.

        \subsection{LFS}
            \label{sec_LFS}

            With advances in software and hardware, by \citeyear{IO_bottleneck}
            file access and reliability patterns became apparent. Especially
            for office use, small reads and writes dominated the workload. Like
            in UFS and FFS, the mechanical nature of hard drives made such
            operations costly when performed in a random manner. To imporove
            performance, caching is identified as a substantial improvement,
            which has been wildly used since \cite{IO_bottleneck}
            \cite{Linux_VFS_cache}.

            However, \citeauthor{IO_bottleneck} points out some issues with
            such an approach: a system crash or a power outage puts the
            contents of such a cache in jeopardy. This is not a problem for a
            read cache, but a write cache loss can be catastraphic. This brings
            reliability into the picture and balancing it with performance
            becomes a key factor.

            Further, \citeauthor{IO_bottleneck} argues that such caching shifts
            the IO pattern from mostly random reads to mostly writes in
            occasional big bursts of relatively large amount of data.
            Maintaining the popular layout of filesystems so far does not make
            use of this fact, so the author suggests structuring the filesystem
            as a log - an append-only structure which wraps around once it
            fills up. This gives locality to files that are used close
            together, utilises the burst nature of writes and recovery is easy
            - only the end may get corrupted \cite{IO_bottleneck}.

            It must be noted that maintaining a fraction of the disk free is
            once again emphasised as critical for good performance. % TODO:
            % check ffs, this is the first time for now

            % TODO: consider explaining how this relates to the ZIL in ZFS
            \citeauthor{LFS}, reiterates the necessity of asynchronous writes
            (and therefore a cache). In his LFS, based on \cite{IO_bottleneck},
            the inode structure is kept exactly the same as in FFS (including
            the data block arrangement), but its layout policy was redically
            different. First, the i-list became a regular file, but instead of
            holding inodes, it held addresses to where they could be found. The
            inode itself and its data are laid our sequentially elsewhere to
            improve performance and the free list is done away with. Instead,
            the disk is split into large continuous chunks of free space -
            \textbf{extents} (analogous to segments in LFS) - which contain a
            single block of metadata that includes that information.  Special
            care is taken to maintain these chunks large. This is done to
            reduce fragmentation and to allow a better likelyhood for
            sequential accesses. This idea for large extents remains with us
            today.

            New data is written in a log fashion. There are several fixed
            places where a pointer to the i-list is stored and there is a
            single fixed place that is dedicated to storing the last known good
            head of the log to ensure reliability.

        \subsection{WAFL}
            \label{sec_WAFL}

            With time reliability became an ever growing concern, both from a
            techincal and from a human perspective. The idea of snapshots
            contributes to both of these.

            Snapshots are read-only copies of the entire file system. WAFL
            \cite{WAFL} keeps many of them, allowing for old versions of files
            to be viewed after deletion. To achieve this it uses copy-on-write
            (CoW) semantics for disk blocks. This means blocks have to be
            copied elsewhere to write to them. Snapshots are essentially
            pointers to blocks. CoW allows a new snapshot to consume extra
            space only for the difference with another snapshot. Block are
            freeded after all references to them have been removed. This has
            the added benefit that it avoids duplicating data to store the
            snapshots.

            Besides file versioning, snapshots are useful for backups on a live
            system, as creating them is pretty much much free (see below).

            The way WAFL implemnts the CoW is by creating a tree of blocks,
            where the root is a known fixed block. Then it has all metadata
            (like the free list) be a file too. The inodes of all of these
            files are stored in the inode list, itself a file, whose inode
            itself is stored in the root block. This way every single bit of
            information required for the filesystem's operation is connected in
            a rooted tree and copying the root block makes a snapshot.
            Modifying any other block requires copying it to a new location
            with the modificaiton only on the new copy and modifying the
            reference to it. Modifying the reference requires the same
            operation, which eventually bubbles up to the root block which can
            be modified in place.

            The benefit of this approach is that the filesystem is always
            consistent. At no point is any data modified in place, meaning a
            crash cannot cause corruption. Rather a new copy is built somewhere
            in free space and only "added" to the filesystem when the whole
            branch of the tree is complete. Writing a single block is assumed
            to be atomic \cite{drive_atomicity} so updating the root is an all
            or nothing operation, limiting the impact of a crash to only some
            data loss that hasn't had the time to be commited to disk.


        \subsection{Other filesystems}
            XFS?

    % TODO: look at my notes from algorithms. Adjust as necessary (I think
    % btrees tend to have values too, so B+trees would have the tree as keys
    % and then values in the leaves?). This would also apply to the htree
    % TODO: very unnatural explanation I think
    \section{The B-tree}
        \label{sec_btree}

        B-trees are a de facto standard for file organisation
        \cite{btree_ubiquitous}. Most modern filesystems incorporate them in
        some way, like ext4 (section \ref{sec_ext4}), XFS
        \cite{XFS_scalability}, HFS+ \cite{HFSplus} and, probably most notably,
        BTRFS which stands for "B-tree File System" \cite{BTRFS}. They are very
        well suited to secondary storage based applications and their use is
        only natural.

        % TODO: prime candidate for graphics
        As explained by \citeauthor{btree_ubiquitous}, B-tree are a
        generalisation of binary search trees. Essentailly, each node contains
        any number of keys, instead of one, and that many plus one children
        leave a node. Keys in a node are ordered and child nodes are placed
        between any two adjacent keys. Additionally, the keys of each child are
        selected such that they fall between the two keys where the node is
        placed. % middle of the sequence of the parents? idk how to say

        B-tree nodes do not have to be full. Indeed, keeping only a single key
        and two children in each devolves in a generic binary truee. However,
        they are usually kept at least $1/2$ full, splitting and combining them
        as necessary to maintain the constraint, a process called balancing.

        Since B-trees are frequently used on secondary storage measuring the
        cost of operations by number of nodes accessed is a typical metric
        \cite{btree_ubiquitous}[ch.2] as this is the slowest part of the
        operation. In that case, all operations are logarithmic in the size of
        the tree (times a different constant depending on the operation).

        A particular variant, the \bplustree only stores keys in leaf nodes and
        inner nodes are only used for path finding. Adjacent leaf nodes are
        also linked together to enable constant time sequential access, as is
        common with files.

        B-tree variants are particularly well suited for secondary storage for
        a variety of reasons.

        As secondary storage is accessed in blocks of fixed size, aligning the
        B-tree node size with it reduces the need for bookkeeping information
        and fragmentation. The relatively large block sizes also allow many
        keys to be stored in a node, allowing for the tree to be much
        shallower, leading to very few random block accesses, an operation
        which is particularly slow on hard drives. Finally, the minimum usage
        requirement for nodes prevents sparse trees from forming, increasing
        space utilisation and further reducing depth.

    \section{ext4}
        \label{sec_ext4}

        The most modern version of the ext filesystem, ext4, is native and
        usually default on Linux. It integrates a lot of the improvements over
        the years while maintaining the same general idea. Since it was
        developed for (and by) Linux, which is a UNIX derivative, the ext
        family follows closely the same principles present over 50 years ago in
        UFS (section \ref{sec_UFS}).

        It is a good benchmark for modern filesystems as it is the default on
        many (if not most) major Linux distributions like Debian
        \cite{Debian_filesystem} and Red Hat \cite{RedHat_filesystem}.  It is
        actively maintained \cite{ext4_mailing_list} and very mature, first
        appearing in version 2.6.19 in 2006 \cite{ext4_origin}.  Finally,
        unlike its most direct competitiors by the likes of Windows' NTFS and
        MacOS' APFS, it is free and open source, so code and documentation are
        readily available.

        % TODO: ext4 extent tree is like a btree. Never said explicitly, cos
        % its funky. You have to explain why (when you draw it on paper it's
        % pretty much that).
        \subsection{The Htree}

            The best data structure for indexing a list of names, such as a
            directory, would be hash table, due to its very fast lookup times
            of arbitrary length strings. However a major drawback is that their
            size is fixed and extending a hash table cannot be done
            incrementally, but rather has to happen in large chunks. However,
            due to the dynamic nature of filesystem directories having fixed
            sized indexes will not satisfy sizes much different than whatever
            the general case might be set. On the other hand rebalancing,
            called rehashing, may lead to large spikes in file creation time, a
            feature that is undesiriable in filesystems. \citeauthor{HTree}
            also presents some implementation difficulties.

            Instead, ext4 uses a modified form of the B-tree known as an Htree
            (meaning Hash tree) \cite{HTree}. The major difference is that keys
            in the tree are hashes of directory entry names instead of the
            names themselves. This way the tree can implement ranges of hashes
            (buckets in the hash table) in its leaves. This solves the problem
            of rehashing the table, by allowing incremental changes.
            Additionally, leaves are kept at a uniform depth to remove the
            impact of rebalancing the tree and maintaining good performance.

        \subsection{Data locality}

            ext4 employs a range of tricks to maximise locality of data of
            various types. This has the obvious benefit of reducing seek times
            on spinning hard drives, but can also help solid state drives too
            as locality "can increase the size of each transfer request while
            reducing the total number of requests" \cite{ext4_docs}.

            % TODO: i haven't checked the references
            First, it uses blocks of at least 4KB, as discussed in chapter
            \ref{sec_FFS}, but even larger (powers of 2) are supported. Then,
            on file creation allocation is speculative and large to give space
            for the file to grow inmake sure that's where it was
            discusseditially. Additionally, allocation is delayed as long as
            possible, to ensure unwritten chunks can be coalesced into larger
            ones and laid out better. It also tries to keep the inode close to
            its data and its parent directoy with the presumption that they are
            related and accessing them sequentially is likely.

            A second trick is that the filesystem is split into large block
            groups in which the above rules apply. A major exception to them is
            that root directoy entries are spread out as much as possible to
            encourage them to grow independent and reduce fragmentation.

            The final interesting technique is that ext4 uses extents which are
            a "range of contiguous physical blocks owned by a file"
            \cite{ext4_space_maps}. Using them allows to reduce the amount of
            necessary metadata for large contiguous chunks of space which
            imporoves performance.

        \subsection{Metadata integrity}

            System crashes are a consistent issue. Causes include dormant bugs
            in the kernel, applications misbehvaing at critical moements or
            even an improper shutdown. Even the best code can be brought down
            by completely unexpectable events like bit flips due to a stray
            gamma ray and many machines lack the hardware capabilities to
            detect or correct them, as evidenced by a recent rant by Linus
            Torvalds \cite{Linus_ECC_rant}.

            To that effect, system crashes are assumed to be inevitable and
            steps are taken to address them. This is especially true for
            filesystems as data loss is one of the most critical, annoying and
            time consuming failures possible. There are various ways to
            minimise the chances of that happening but two major ones are Soft
            updates, where metadata is only written in a very specific order to
            ensure consistency \cite{soft_updates} and journaling where
            metadata is written to a small LFS style log before entering the
            filesystem proper \cite{ext4_docs}[journal].
            \citeauthor{journaling_vs_soft_updates} presents a thorough
            comparison but in the EXT familiy journaling is used.

            Another reliability feature is that all ext4 data structures are
            checksummed wherever possible (due to backwards compatibility).
            This greatly helps with combating silent filesystem corruption and
            can help the kernel limit damage.

            It is important to note that despite the reliability features of
            ext4, only metadata enjoys their guarantees. Data itself (like file
            contents) have no guarantees whatsoever. It is impossible to enable
            journaling for data too, however, the performance impact is sever
            \cite{ext4_docs}[journaling]. Adding other guarantees for it
            requires backwards incompatible changes to the filesystem, which
            the Linux kernel is famously unfavourable towards with the "Never
            break userspace" motto \cite{never_break_userspace}

    \section{Data integrity}

        % TODO: validate from paper, becase most of this is my general
        % knowledge (espcially parity calculation)
        \subsection{RAID}

            Non-volatile storage mediums have historically been orders of
            magnitude slower than volatile memories like RAM which are
            themselves much slower than CPUs and their caches
            \cite{IO_bottleneck}. Disk arrays are an attempt to increase IO
            performance and bridge that gap. However, drives are inherently
            unreliable \cite{Backblaze_stats}, so putting many of them together
            exacerbates the problem \cite{RAID}. RAID is one solution for the
            problem of drive reliability \cite{RAID}. As its name suggests,
            RIAD adds redundancy to disk arrays. Popular RAID arrangements are
            RAID 1 and RAID 4/5.

            RAID 1 is two drives acting as one, effectively a mirror. If one of
            them fails the other is a perfect copy.

            RAIDs 4 and 5 are very similar. Both calculate extra parity for
            each block (by XORing the respective blocks on all disks together)
            and store it on an extra disk. The major difference is how they lay
            out parity across the disks. The result is that the array can lose
            a disk any of the disks while keeping all of the data intact. It
            has a benefit over RAID 1 in that it is cheaper.

            Even though not part of the original paper, RAID 6 is also very
            common. It is essentially RAID 5 but with more copies of the parity
            data (equal to the number of parity disks).

            % TODO: non standard raid levels (specifically RAID-Z) OR in design have a citation for raid-z

        \subsection{drive reliability}
            \label{sec_reliability}

            Drives are unreliable \cite{RAID}, \cite{Backblaze_stats}. Due to
            their mechanical nature they wear out and stop working. They do
            this at an unpredictable rate for individual drives, but at
            relatively constant rates over large arrays \cite{Backblaze_stats}.
            Tools for monitoring drive health, like SMART, have been developed
            to combat the issue and are in general a good indicator for the
            likelyhood of loss of the drive.

            However, drive failure is not binary. They do not work flawlessly
            until such a time as to not work at all \cite{2D_RAID}. Individual
            sectors develop errors over time, failing to be read or written to.
            Drives try to combat the issue by keeping an array of spare sectors
            that replace misbehvaing ones when detected. To determine if a
            sector is misbehvaing in the first place, drives employ ECC codes
            \cite{data_corruption_storage_stack}.
            % TODO: ^ citation might be bad

            However, despite all the best efforts of drives and their
            manufacturers, drives still cannot be relied upon to report correct
            data all of the time \cite{data_corruption_storage_stack}.
            Occasionally, they will report an unrecoverable error to the
            operating system. Even more rarely, drives will return incorrect
            data without indicating errors. As a consequence, individual drives
            are never relied upon for the data to be present and intact and
            other techniques are always employed to ensure both of these
            independently lest there be data loss \cite{LTT_data_loss}.

            SSDs and flash storage in general are a similar tale. As
            \citeauthor{2D_RAID} puts it: "Flash is basically garbage. It
            barely works. We tolerate it because its fast" \cite{2D_RAID}. They
            employ similar techniques as spinning hard drives and suffer
            similar problems as them \cite{flash_large_scale},
            \cite{flash_reliability}. Their failure patterns are substantially
            different but the end result is the same - unrecoverable errors
            that would lead to data loss if not accounted for outside the
            drive.

        \subsection{At scale}

            A unique case for data reliability are datacenters for which data
            loss is not an option. They also operate at massive scales, where
            conventional approaches break down or become severe bottlenecks.
            Hence many, if not most, of these companies have special bespoke
            arrangements to fulfil their needs.

            \subsubsection{Backblaze}

                Conveniently, Backblaze, a cloud storage provider, has an open
                approach to its infrastructure, frequently publishing in-depth
                information and open sourcing parts of their infrastructure.

                Their solution \cite{Backblaze_arch} to reliability is
                conceptually distributed RAID 6 over 20 drives, 3 of which are
                parity. However, they do not use conventional RAID. Instead,
                they use custom software to split files into chunks they call
                "shards" which are then stored on normal ext4 filesystems. The
                choice is motivated by the good reliability and performance of
                ext4 and that inverting the usual order of replication and
                filesystem allows them to protect against filesystem
                corruption.

                Additionally, every shard is checksummed to protect against
                data corruption. This is necessary because "once in a while
                they return the wrong data, or are just unable to read a given
                sector" \cite{Backblaze_arch} and software needs to be able to
                identify the error and recompute the data from the parity.

            \subsubsection{GFS}

        \subsection{Caching}

            % TODO: you can't not cite this. LTT could be a source?
            % TODO: soft-updates seems to be part of caching system. Explain
            % modern caches and that they have been there since time immemorial

            Caching is an integral part of modern operating systems
            \cite{IO_bottleneck} \cite{LFS} \cite{FFS}. All modern variants
            employ it in various very visible ways. Reads are routinely cached
            and so are writes although with extra care. This practice is so
            prevalent that opening files/programs the first time is frequently
            orders of magnitude slower than any subsequent access. In
            particular, it is essential in obtaining good, almost sequential,
            performance out of random small writes \cite{soft_updates}.

            Caching, however, is tricky especially on the write side. Besides
            the usual cache coherence issues, caching of writes for prologned
            periods of time can easily lead to data loss. To avoid this
            possibility while not diminishing the effectiveness of techniques
            like soft updates \cite{soft_updates}, operating systems have to
            perform a fine balancing act.

            Linux in particular caches aggresively \cite{Linux_VFS_cache}.
            Luckily, its VFS layer implements this in an abstract manner, so
            individual filesystems do not need to implement it themselves.
            Nevertheless, they frequently need to be aware of it and in rare
            cases, like ZFS (\ref{sec_ZFS}), they need special behaviour to
            implement some features of the filesystem.

        % TODO: incorporate the reliability analysis of ZFS here
        \subsection{ZFS}
            \label{sec_ZFS}

            Considered by many to be the most robust enterprise ready
            filesystem (like TrueNAS does \cite{TrueNAS_enterprise}), ZFS takes
            all necessary aspects of a filesystem and tries to combine them
            into one as close to bulletproof solution. It is widely recommended
            in place of almost any other solution where possible. It integrates
            a very wide variety of features to ensure data integrity while
            maintaining performance.

            % The data blocks contain objects. Each object contains different
            % things: an inode, an inode's data, a directoy and so on.
            In ZFS all blocks are checksummed. This checksum is stored on any
            other block that saves a pointer to it. This way data and error
            correction metadata are separated and the likelyhood of
            simultaneous corruption of both is reduced. Blocks are chained in a
            tree. The root of the tree is called the uberblock. Leaves of the
            tree contain (arbitrary) data. The uberblock stores its own
            checksum but is the only one to do so. To mitigate it getting
            corrupted, several backup copies of it are stored.

            Modifications to the tree of blocks is done in the same copy on
            write manner as in WAFL (\ref{sec_WAFL}) so it is always
            consistent.  The only critical operation is the uberblock
            overwrite, but its checksum allows for any failure to be detected
            and a backup read.

            To pave the way for efficient multi-drive arrangements, ZFS
            implements pools (effectively a volume manager). A pool is an array
            of vdevs (virtual devices) which provide storage. Their space is
            concatenated together and handed out to every different volume
            (what ZFS calls an individual filesystem) on a per-reqest basis.

            Vdevs exist to provide (completely optional) redundancy. A vdev is
            a node with 1 or more children. Children can be either drives or
            other vdevs. Vdevs come in different types to implement various
            features: for example a mirroring vdev mirrors its IO to both
            children. Alternatively a vdev can implement RAID 5, concatenation
            or any other useful operation. Since children can be either drives
            or other vdevs, vdevs allow for drive trees of arbitrary complexity
            and depth. Since pools concatenate vdevs, different drives can be
            grouped and different groups can have different redundancy
            configurations, drive sizes and types.

            Even though ZFS also goes to great lengths to allocate data blocks
            in contiguous chunks of space, ZFS reads and writes tend to be
            slower than those of other filesystem even when paired with RAID
            functions due to the copy on write "bubbling" mechanism having to
            overwrite many blocks \cite{ZFS} \cite{WAFL}. To counteract this
            ZFS caches extensively. So much so that TrueNAS (a popular home
            file server operating system) has a dedicated guide on how much
            memory a server should have
            \cite{TrueNAS_hardware_guide}[memory-sizing] and more memory is
            frequently recommended on forums. The caching is such an important
            part of ZFS that it does not rely on the operating system.

            % You might want to rephrase the ARC citation to be more in line
            % with the paper
            ZFS caching has 2 important parts: The ARC and the ZIL.

            As the name suggests, the ARC is a cache that dynamically adjusts
            its caching policy to current usage patters to maximise cache hits
            \cite{ARC}. It even allows a second level of caching (the L2ARC) on
            high speed storage for data that is not top caching priority but is
            accessed frequently enough to be kept around. The ARC is necessary
            due to the tree nature of the filesystem. To verify the integrity
            of blocks many indirect accesses are required which degrades
            performance. To ensure that the same guarantees as data on disk are
            extended to the contents of the ARC, use ECC memory is strongly
            recommended too \cite{TrueNAS_hardware_guide}[error correcting code
            memory]

            The ZFS Intent Log, ZIL, acts like and solves similar problems to
            LFS (\ref{sec_LFS}). ZFS is always consistent on disk but to due to
            its copy on write nature, performing these updates frequently
            requires many bubbling operations up to the uberblock degrades
            performance.  The ZIL acts as a stopgap for data that has been
            accepted, cached and not flushed to disk yet. It is a log, similar
            to LFS, that records all writes in sequence until they can be
            commit on disk. In the event of system failure data uncommited
            writes can be recovered.

            In summary, ZFS tries to account for \textit{every} eventuality and
            makes arrangements to mitigate them. In places where this affects
            performance, further steps are taken to minimise the impact of
            this, such as various caching. The result is a watertight design
            which is infamous for reliability across the industry.

\chapter{Problem setting}

    As we have seen, non-volatile storage devices are very error-prone and not
    particularly reliable (\ref{sec_reliability}). To account for this, modern
    filesystems, like ext4 (\ref{sec_ext4}), perform metadata checksumming to
    avoid filesystem corruption. However, this still leaves actual data in the
    filesystem vulnerable. There exist various methods to combat the issue,
    like disk based parity RAID (\ref{sec_RAID}) or dedicated filesystems like
    ZFS (\ref{sec_ZFS}). These solutions are not perfect and suffer from some
    unfortunate flaws. There are notable examples of data loss even when using
    them \cite{LTT_data_loss}.

    \section{The write hole}
        \label{sec_RAID_problems}

        RAID5 has a critical issue: writing data and experiencing a system
        crash can (silently) corrupt unrelated data on the disk. This
        phenomenon is known as the write hole \cite{LWN_md_journal}.

        The problem arises from the fact that writing a single block or stripe
        of data on a RAID5 array requires at least two writes to two
        independent drives. The first is the actual block where the data is
        stored and the second is its parity computed from it and all asociated
        blocks on other drives. If the system were to experience any kind of
        failure in between these two writes then the array is left in an
        inconsistent state. The data on the non-parity disks is correct but
        their parity is not. However, when the system detects the issue it
        cannot repair it correctly. When parity is recalculated it will not
        match the one on disk, which is an older version. Alternatively, the
        system can recompute any of the associated blocks it might believe to
        be the issue. That can either be the block written just before the
        failure, resulting in reverting the write, or any of the related
        blocks, resulting in overwriting correct data. Even worse, if any of
        the drives in the array dies, then one of these will happen silently,
        depending on which drive fialed.

        TODO:
        Explain the extreme case: a RIAD 1 mirror. If one is (silently) wrong,
        which one can be trusted? Neither.

        The problem lies in the fact that the system cannot distinguish between
        these three options and the second one leads to data loss. The maths
        works in any of the three configurations, depending on which block we
        choose to be unknown and the hardware will report all of them as
        equally functional (in the general case). What is more, the dataloss is
        completely unrelated to the file, so a filesystem journal cannot hope
        to correct the issue.

        To "close" the write hole, Linux implements two features: a journal for
        RAID \cite{LWN_md_journal} (also called a cache occasionally) and a
        partial parity log (PPL) \cite{partial_parity_log}.
        The journal uses a separate drive to store a journal of writes, much
        like the ZIL and ext4 (\ref{sec_ext4}, \ref{sec_ZFS}) and then recover
        any corruption on startup. Alternatively, the PPL stores extra
        (partial) parity in the metadata area of parity drives, again allowing
        for recovery on startup.

        Both of theses are imperfect solutions though. The journal requires an
        additional drive (however small) which is very cumbersome for small
        systems like small form factor home servers and laptops (although the
        advent of technologies like M.2 Intel Optane \cite{Intel_Optane} has
        begun alleviating that concern too but it is expensive and an abundance
        of M.2 slots, which tend to be used for boot drives, is not widepsread
        yet). Regardless, the journal is susceptible to the same issues as the
        drives it is trying to protect.  So inevitable errors in the log drive
        (\ref{sec_reliability}) can diminish its benefit.

        The PPL is imperfect too: its documentation \cite{partial_parity_log}
        concedes it reduces performance by up to 40\% and in the event of a
        loss of a (dirty) drive its behaviour is the same as unaltered RAID.

    % TODO: incorporate the reliability analysis of ZFS here
    % Does this go here?
    \section{Scrubbing}

        % \cite{https://www.truenas.com/docs/core/tasks/scrubtasks/}
        Drive scrubbing is a necessary feature for both hardware (RAID) and
        filesystem (ZFS) based solutions. It is essentially a regularly
        scheduled automatic full filesystem read that is meant to detect and
        repair silent errors (\ref{sec_reliability}) such that in the event of
        a loss of a drive data is still recoverable and does not cascade into
        more data loss. It is a simple but critical part of reliable data
        storage. The obvious drawback is that this induces significant wear on
        drives, but the benefit greatly outweighs this. If scrubbing is not
        regularly performed, loss of drives can mean data loss is inevitable no
        matter the redundancy solution that is being used. Even a filesystem
        like ZFS with incredible reliability characteristics
        \cite{ZFS_reliability} is vulnerable to this. If scrubbing is omitted
        data corruption may be silent in the case of RAID or more detectible in
        the case of ZFS.

        This technicality was recently demonstrated by a popular tech channel
        \cite{LTT_data_loss}. Due to no scheduled scrubbing and no drive
        failure notifications they experienced major and unrecoverable data
        loss on a storage array of over a petabyte.

    \section{Drive failure notifications?}

    \section{3-2-1 rule}

    \section{ZFS resources}

        The reliability of ZFS comes at a very steep price - system resources
        \ref{sec_ZFS}. Due to its caching policy, main memory requirements are
        very high and not satisfying them can and will degrade performance.
        Workstation class devices are not dedicated file servers and rely on
        their memory for other tasks - 3D morelling, rendering, compillation
        and so on. Increasing the memory requirement can be impossible for
        laptops. At the same time, they do need the speed and reliability for
        doing work effectively, as data loss can be catastrophic.

        Alternatively, for small home servers the extra cost for memory may be
        unjustifiable for a small box with a low-end CPU and a hadnful of
        drives. Features like the ZIL may not be incorporated too, further
        diminishing the benefit.

        All of this is ignoring the fact that ZFS tends to run best on
        dedicated or more specialist operating systems like TrueNAS or Linux
        which may be incompatible with home Windows based machines or less
        techincally savvy users.

    \section{The problem to address (conclusion)}
        \label{sec_problem}

        Drives cannot be relied upon, no matter if mechanical or solid state.
        It is evident that there exist a plethora of techniques to ensure data
        integrity on a wide range of operating systems for all storage segments
        despite these shortcomings. However, there exists a reliability gap for
        small scale personal computers, workstations and home servers.
        Filesystems that are intended for them, like ext4, do not provide
        sufficent data integrity guarantees. Those that do can, like ZFS, be
        overkill or simply financially infeasible. Simple solutions, like RAID,
        have significant flaws by default and addressing them presents similar
        challenges as to ZFS. Therefore a minimal filesystem with ext4 like
        features and basic amounts of redundancy to provide small scale data
        guarantees is missing while there could be significant benefits from
        using one.
