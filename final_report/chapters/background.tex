% You can cite chapters by using '\ref{chapter1}', where the label must
% match that given in the 'label' command, as on the next line.
\label{background}
% Sections and sub-sections can be declared using \section and \subsection.
% There is also a \subsubsection, but consider carefully if you really need
% so many layers of section structure.
\chapter{Introduction and Background Research}

    \section{Introduction}

    % <A brief introduction suitable for a non-specialist, {\em i.e.} without
    % using technical terms or jargon, as far as possible. This may be
    % similar/the same as that in the 'Outline and Plan' document. The
    % remainder of this chapter will normally cover everything to be assessed
    % under the `Background Research` criterion in the mark scheme.>

    % Here is an example of how to cite a reference using the numeric system
    % \cite{parikh1980adaptive}.


    % Must provide evidence of a literature review. Use sections
    % and subsections as they make sense for your project.

% TODO: this is a temporary name
\chapter{Lit review}
    \section{The UNIX filesystem}
        \label{UFS}

        For the past 50 years the UNIX style operating system has been one of
        the most prevalent operating system architectures out there. The
        original UNIX brought about a simple and elegant template which is
        still with us to this day. It brough forward many novel ideas: wide
        hardware compatibility, simple and powerful system calls, a powerfull
        shell and minimal design and most importantly a file system
        \cite{UNIX}. All these features are still with us, largely unmodified,
        to this day, although greatly expanded and improved upon.

        The most important part, the filesystem, had an implementation akin to
        its interface - simple and elegant. It introduced several important
        concepts:

        From a programming view, a file is a linear sequence of bytes and can
        be controlled with just 4 system calls: \syscall{open} \syscall{read},
        \syscall{write} and \syscall{close}. No structure is enforced on it
        from the operating system. A file can be \syscall{open}ed by name to
        get a handle to it. Programs can \syscall{read} all of its contents and
        \syscall{write} back anything new. The operating system maintains an
        offset showing the current location within the file, so sequential
        calls to \syscall{read} and \syscall{write} begin operation from that
        location and increment it by the amount they modified. No limit is
        imposed on file size, so writing past the end of the file increases it.
        A file must be \syscall{close}ed after use.

        From an implementation view, every file has an i-number. This is a
        number that uniquely identifies this file, regardless of its path or
        location. It is essentially the pointer to the file in the table of
        files, called an i-list (or i-table more recently).

        Each entry in the i-list is an i-node (modernly shortened to just
        inode). The inode is essentially the header for the file itself, it has
        all metadata including pointers to the actual data.

        To make this flat list of files into a usable tree, special files are
        introduced, called directories, marked by a special bit in the inode.
        The directory is a file whose contents have a special format: a list of
        2-tuples (name, i-number), which denote that file number
        \textit{i-number} can be found in this directory under name
        \textit{name}. Directories are special in that they cannod be read and
        written directly, rather special system calls must be used for the
        operating system to report its contents. The exact method is slightly
        involved and not relevant for this dissertation, but it is sufficent to
        say that there are simple library functions roughly equvalent to those
        for normal files.

        File names are alphanumeric strings of any length. To denote that a
        file is contained within a directory, a single forward slash ("/") is
        used, meaning whatever is on the right of it is contained on the
        directory named on the left. This can be repeated an unlimited number
        of times. The root directory has no name \textit{per se}, hence any
        path refering to it starts with a slash, for example
        \textit{/file/in/root}.

        File number 1 is defined to be the the root directory to bootrstrap the
        search and inode 0 does not exist.

        An important, but not user visible detail (in any way) is that the
        filesystem has a list of free blocks (called the free list), to denote
        which space is unused by the system, where a block is a fixed chunk of
        space on disk.


    \section{Evolution of implementation}

        \subsection{UFS}

            Initially, this model had a very simple implementation. The
            original UNIX used a linear, fixed size i-list. An inode had 13
            pointers to data blocks, 10 of which point directly to data blocks,
            while the next 3 have 1, 2 and 3 levels of indirection
            respectively. The free list was a simple linked list, where
            each block had pointers to free blocks and a pointer to the
            next block in the list \cite{UNIX_implementation},
            \cite{UNIX_thompson}.

            Over time, however, it turned out that this arrangement was very
            inefficient. In particular, data blocks for a file lacked locality,
            as did the i-numbers for files in the same directory. Additionally,
            the free list would become very scrambled in that sequential
            entries in the list may be wildly far apart on disk. All of these
            problems got exacerbated after prologned use. On top of all that,
            due to hard drives (the main storage medium) being a physical
            spinning platter with a head that has to move to the track on drive
            and wait for the block to come under it (operation knows as
            "seek"), seek times for arbitrary blocks was large. As a result,
            throughput could be as low as 4\% \cite{FFS}.


        \subsection{FFS}

            % maybe mention that increasing block size decreases the need for
            % indirection
            \citeauthor{FFS} proposed many improvements to improve performance.
            For a start, block size was increased 8 fold from 512 bytes to 4096
            (or even 8192), which improved locality and reduced seek times,
            essentially for free, although extra complexity was introduced to
            minimise wasted space from small files. Further, blocks were
            grouped together into cylinder groups, where a cylinder is a
            physical feature of a hard drive, blocks on which have very good
            sequential performance.  Then, policies were introduced to keep
            data blocks of a file and inodes in a directoy sequential and on
            the same cylinder group where possible. Finally, the free list
            became a bitmap of a fixed size, eliminating the scrambling of
            supposedly sequential entries that further degrade performance.

            The result was that \citeauthor{FFS} measured an increase in disk
            bandwidth utilisation of about an order of magnitude without an
            increase in wasted space.

            % TODO: do i need this
            % An important note is that cylinder groups include redundant copies
            % of various metadata to increase resiliance to data loss from damage
            % to the disk.

            % Another important note is that filesystems which are near their
            % capacity suffer greatly decreased performance.

        \subsection{LFS}
            \label{LFS}

            With advances in software and hardware, by \citeyear{IO_bottleneck}
            file access and reliability patterns became apparent. Especially
            for office use, small reads and writes dominate the workload. The
            physical nature of spinning hard drives make such operations costly
            when performed in a random manner. To imporove performance, caching
            is identified as a substantial improvement, which has been wildly
            used since \cite{Linux_caching}, \cite{IO_bottleneck}.


            However, \citeauthor{IO_bottleneck} points out some issues with
            such an approach: a system crash or a power outage puts the
            contents of such a cache in jeopardy. This is not a problem for a
            read cache, but a write cache loss can be catastraphic. This brings
            reliability into the picture and balancing performance for
            reliability becomes a key factor.

            Further, \citeauthor{IO_bottleneck} argues that such caching shifts
            the IO pattern from mostly random reads to mostly writes in
            occasional big bursts of relatively large amount of data.
            Maintaining the usual layout of filesystems popular so far does not
            make use of this fact, so the author suggests structuring the
            filesystem as a log - an append-only structure which wraps around
            once it fills up. This gives locality to files that are used
            together, utilises the burst nature of writes and recovery is easy
            - only the end may be corrupted \cite{IO_bottleneck}.

            It must be noted that maintaining a fraction of the disk free is
            once again emphasised as critical for good performance.

            % TODO: consider explaining how this relates to the ZIL in ZFS
            \citeauthor{LFS}, reiterates the necessity of asynchronous writes
            (and therefore a cache). In LFS \cite{LFS}, based on
            \cite{IO_bottleneck}, the inode structure is kept exactly the same
            as in FFS (including the data block arrangement), but its layout
            policy was redically different. First, the i-list is now a regular
            file, but only contains addresses to where the actual inode can be
            found. The inode itself and its data are laid our sequentially to
            improve performance and the free list is done away with. Instead,
            the disk is split into large chunks of free continuous space -
            \textbf{extents} (called segments in LFS) - which contain a single
            block of metadata that includes that information.  Special care is
            taken to maintain these chunks large.  This is done to reduce
            fragmentation and to allow a better likelyhood for sequential
            accesses. It is important to note, that the idea for extents
            remains present to this day.

            New data is written in a log fashion. There are several fixed
            places where a pointer to the i-list is stored and there is a
            single fixed place that is dedicated to storing the last known good
            head of the log. This ensures reliability.

        \subsection{WAFL}
            \label{WAFL}

            With time reliability becomes an ever growing concern, both from a
            techincal and from a human perspective. The idea of snapshots
            contributes to both of these.

            Snapshots are read-only copies of the entire file system. Many of
            them are kept, allowing for old versions of files to be viewed in
            case of deletion. WAFL uses copy-on-write for disk blocks to avoid
            duplicating data \cite{WAFL}. Snapshots are essentially pointers to
            these blocks, hence a new snapshot consumes space only after data
            is modified. Block are freeded after all references to them are
            freed too.

            Besides file versioning, snapshots are useful for backups on the
            live system, since they are pretty much free on WAFL.


            The way WAFL implemnts the copy-on-write is by creating a tree of
            blocks, where the root is a known fixed block. Then it has all
            metadata (like i-list and free list) be a file too. The inodes of
            all of these files are stored in an inode file, whole inode itself
            is stored in the root block. This way every single bit of info
            required for the filesystems operation is connected in a rooted
            tree and copying the root block makes a snapshot. Modifying any
            other block requires copying to a new location with the
            modificaiton only on the new copy and modifying the reference to
            it. Modifying the reference requires the same operation, which
            eventually bubbles up to the root block which can be modified in
            place.

            The benefit of this approach is that the filesystem is always
            consistent. At no point is any data modified in place, meaning a
            crash cannot cause corruption. Rather a new copy is built somewhere
            in free space and only "added" to the filesystem when the whole
            branch of the tree is complete. Writing a single block is assumed
            to be atomic \cite{???}, so updating the root an all or nothing
            operation, limiting the impact of a crash to only some data loss
            that hasn't had the time to be commited to disk.

    \section{EXT4}

        The most modern version of the ext filesystem, ext4, is native and
        usually default on Linux. It integrates a lot of the improvements over
        the years while maintaining the same general idea. Since it was
        developed for (and by) Linux, which is a UNIX derivative, the ext
        family follows closely the same principles present over 50 years ago in
        UFS (section \ref{UFS}).

        It is a good benchmark for modern filesystems as it is the default on
        many (if not most) major Linux ditributions
        \cite{https://wiki.debian.org/FileSystem},
        \cite{https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/performance_tuning_guide/s-storage-fs}.
        It is actively maintained
        \cite{https://www.spinics.net/lists/linux-ext4/} and very mature, first
        appearing in version 2.6.19 in 2006
        \cite{http://www.h-online.com/open/features/Kernel-Log-Higher-and-Further-The-innovations-of-Linux-2-6-28-746805.html}.
        Finally, unlike its most direct competitiors by the likes of Windows'
        NTFS and MacOS' APFS, it is free and open source, so code and
        documentation are readily available.
    \section{Data integrity}

        % TODO: validate from paper, becase most of this is my general
        % knowledge (espcially parity calculation)
        \subsection{RAID}

            Non-volatile storage mediums have historically been orders of
            magnitude slower than volatile memories like RAM which are
            themselves much slower than CPUs and their caches \cite{raid has a
            Frank87, Stevens81}. Disk arrays are an attempt to increase IO
            performance and bridge that gap. However, drives are inherently
            unreliable \cite{pls (backblaze is a killer source)}, so putting
            many of them together exacerbates the problem \cite{maybe also
            raid. he has an MTTF caluclation}.  RAID is one solution for the
            problem of drive reliability \cite{RAID}. As its name suggests,
            RIAD adds redundancy to disk arrays. Popular RAID arrangements are
            RAID 1 and RAID 4/5.

            RAID 1 is two drives acting as one, effectively a mirror. If one of
            them fails the other is a perfect copy.

            RAIDs 4 and 5 are very similar. Both calculate extra parity for
            each block (by XORing the respective blocks on all disks together)
            and store it on an extra disk. The major difference is how they lay
            out parity across the disks. The result is that the array can lose
            a disk any of the disks while keeping all of the data intact. It
            has a benefit over RAID 1 in that it is cheaper.

            Even though not part of the original paper, RAID 6 is also very
            common. It is essentially RAID 5 but with more copies of the parity
            data (equal to the number of parity disks).

        \subsection{drive reliability}
            \label{reliability}

            Drives are unreliable \cite{RAID}, \cite{Backblaze (idk which one)}.
            Due to their mechanical nature they wear out and stop working. They
            do this at an unpredictable rate for individual drives, but at
            relatively constant rates over large arrays \cite{Backblaze_stats}.
            Tools for monitoring drive health, like SMART, have been developed
            to combat the issue and are in general a good indicator for the
            likelyhood of loss of the drive.

            However, drive failure is not binary. They do not work flawlessly
            until such a time as to not work at all \cite{2D_RAID}. Individual
            sectors develop errors over time, failing to be read or written to.
            Drives try to combat the issue by keeping an array of spare sectors
            that replace misbehvaing ones when detected. To determine if a
            sector is misbehvaing in the first place, drives employ ECC codes
            \cite{data_corruption_storage_stack, although maybe someone else}.

            However, despite all the best efforts of drives and their
            manufacturers, drives still cannot be relied upon to report correct
            data all of the time \cite{data_corruption_storage_stack}.
            Occasionally, they will report an unrecoverable error to the
            operating system. Even more rarely, drives will return incorrect
            data without indicating errors. As a consequence, individual drives
            are never relied upon for the data to be present and intact and
            other techniques are always employed to ensure both of these
            independently \cite{LTT_data_loss?}.

            SSDs and flash storage in general are a similar tale. As
            \citeauthor{2D_RAID} puts it: "Flash is basically garbage. It
            barely works. We tolerate it because its fast" \cite{2D_RAID}. They
            employ similar techniques as spinning hard drives and suffer
            similar problems as them \cite{flash_large_scale},
            \cite{flash_reliability}. Their failure patterns are substantially
            different but the end result is the same - unrecoverable errors
            that would lead to data loss if not accounted for outside the
            drive.

        \subsection{At scale}

            A unique case for data reliability are datacenters for which data
            loss is not an option. They also operate at massive scales, where
            conventional approaches break down or become severe bottlenecks.
            Hence many, if not most, of these companies have special bespoke
            arrangements to fulfil their needs.

            \subsubsection{Backblaze}

                Conveniently, Backblaze, a cloud storage provider, has an open
                approach to its infrastructure, frequently publishing in-depth
                information and open sourcing parts of their infrastructure.

                Their solution \cite{Backblaze_arch} to reliability is
                conceptually distributed RAID 6 over 20 drives, 3 of which are
                parity. However, they do not use conventional RAID. Instead,
                they use custom software to split files into chunks they call
                "shards" which are then stored on normal ext4 filesystems. The
                choice is motivated by the good reliability and performance of
                ext4 and that inverting the usual order of replication and
                filesystem allows them to protect against filesystem
                corruption.

                Additionally, every shard is checksummed to protect against
                data corruption. This is necessary because "once in a while
                they return the wrong data, or are just unable to read a given
                sector" \cite{Backblaze_arch} and software needs to be able to
                identify the error and recompute the data from the parity.

        \subsection{Caching}

            % TODO: you can't not cite this. LTT could be a source?
            % TODO: soft-updates seems to be part of caching system. Explain
            % modern caches and that they have been there since time immemorial

            Caching is an integral part of modern operating systems
            \cite{IO_bottleneck?}. All modern variants employ it in various
            very visible ways. Reads are routinely cached and so are writes
            although with extra care. This practice is so prevalent that
            opening files/programs the first time is frequently orders of
            magnitude slower than any subsequent access. In particular, it is
            essential in obtaining good, almost sequential, performance out of
            random small writes \cite{soft_updates}.

            Caching, however, is tricky especially on the write side. Besides
            the usual cache coherence issues, caching of writes for prologned
            periods of time can easily lead to data loss. To avoid this
            possibility while not diminishing the effectiveness of techniques
            like soft updates \cite{soft_updates}, operating systems have to
            perform a fine balancing act.

            Linux in particular caches aggresively \cite{its docs?}. Luckily,
            its VFS layer implements this in an abstract manner, so individual
            filesystems do not need to implement it themselves. Nevertheless,
            they frequently need to be aware of it and in rare cases, like ZFS
            (\ref{ZFS}), they need special behaviour to implement some features
            of the filesystem.

        \subsection{ZFS}
            \label{ZFS}

            Considered by many to be the most robust enterprise ready
            filesystem \cite{pls you have to}, ZFS takes all necessary aspects
            of a filesystem and tries to combine them into one as close to
            bulletproof solution. It is widely recommended in place of almost
            any other solution where possible. It integrates a very wide
            variety of features to ensure data integrity while maintaining
            performance.

            % The data blocks contain objects. Each object contains different
            % things: an inode, an inode's data, a directoy and so on.
            In ZFS all blocks are checksummed. This checksum is stored on any
            other block that saves a pointer to it. This way data and error
            correction metadata are separated and the likelyhood of
            simultaneous corruption of both is reduced. Blocks are chained in a
            tree. The root of the tree is called the uberblock. Leaves of the
            tree contain (arbitrary) data. The uberblock stores its own
            checksum but is the only one to do so. To mitigate it getting
            corrupted, several backup copies of it are stored.

            Modifications to the tree of blocks is done in the same copy on
            write manner as in WAFL (\ref{WALF}) so it is always consistent.
            The only critical operation is the uberblock overwrite, but its
            checksum allows for any failure to be detected and a backup read.

            To pave the way for efficient multi-drive arrangements, ZFS
            implements pools (effectively a volume manager). A pool is an array
            of vdevs (virtual devices) which provide storage. Their space is
            concatenated together and handed out to every different volume
            (what ZFS calls an individual filesystem) on a per-reqest basis.

            Vdevs exist to provide (completely optional) redundancy. A vdev is
            a node with 1 or more children. Children can be either drives or
            other vdevs. Vdevs come in different types to implement various
            features: for example a mirroring vdev mirrors its IO to both
            children. Alternatively a vdev can implement RAID 5, concatenation
            or any other useful operation. Since children can be either drives
            or other vdevs, vdevs allow for drive trees of arbitrary complexity
            and depth. Since pools concatenate vdevs, different drives can be
            grouped and different groups can have different redundancy
            configurations, drive sizes and types.

            Even though ZFS also goes to great lengths to allocate data blocks
            in contiguous chunks of space, ZFS reads and writes tend to be
            slower than those of other filesystem even when paired with RAID
            functions \cite{i'm pretty sure someone somewhere said this}. To
            counteract this ZFS caches extensively. So much so that TrueNAS (a
            popular home file server operating system) has a dedicated guide on
            how much memory a server should have
            % \cite{https://www.truenas.com/docs/core/gettingstarted/corehardwareguide/\#memory-sizing}
            and more memory is frequently recommended on forums. The caching is
            such an important part of ZFS that it does not rely on the
            operating system.

            % You might want to rephrase the ARC citation to be more in line
            % with the paper
            ZFS caching has 2 important parts: The ARC and the ZIL.

            As the name suggests, the ARC is a cache that dynamically adjusts
            its caching policy to current usage patters to maximise cache hits
            \cite{ARC}. It even allows a second level of caching (the L2ARC) on
            high speed storage for data that is not top caching priority but is
            accessed frequently enough to be kept around. The ARC is necessary
            due to the tree nature of the filesystem. To verify the integrity
            of blocks many indirect accesses are required which degrades
            performance. To ensure that the same guarantees as data on disk are
            extended to the contents of the ARC, use ECC memory is strongly
            recommended too
            % \cite{https://www.truenas.com/docs/core/gettingstarted/corehardwareguide/\#error-correcting-code-memory}

            The ZFS Intent Log, ZIL, acts like and solves similar problems to
            LFS (\ref{LFS}). ZFS is always consistent on disk but to due to its
            copy on write nature, performing these updates frequently requires
            many bubbling operations up to the uberblock degrades performance.
            The ZIL acts as a stopgap for data that has been accepted, cached
            and not flushed to disk yet. It is a log, similar to LFS, that
            records all writes in sequence until they can be commit on disk. In
            the event of system failure data uncommited writes can be
            recovered.

            In summary, ZFS tries to account for \textit{every} eventuality and
            makes arrangements to mitigate them. In places where this affects
            performance, further steps are taken to minimise the impact of
            this, such as various caching. The result is a watertight design
            which is infamous for reliability across the industry.

\chapter{Problem setting}

    As we have seen, non-volatile storage devices are very error-prone and not
    particularly reliable (\ref{reliability}). To account for this, modern
    filesystems, like ext4 (\ref{EXT4}), perform metadata checksumming to avoid
    filesystem corruption. However, this still leaves actual data in the
    filesystem vulnerable. There exist various methods to combat the issue,
    like disk based parity RAID (\ref{RAID}) or dedicated filesystems like ZFS
    (\ref{ZFS}). These solutions are not perfect and suffer from some
    unfortunate flaws. There are notable examples of data loss even when using
    them \cite{LTT_data_loss}.

    \section{The write hole}

        RAID5 has a critical issue: writing data and experiencing a system
        crash can (silently) corrupt unrelated data on the disk. This
        phenomenon is known as the write hole \cite{LWN_md_journal}.

        The problem arises from the fact that writing a single block or stripe
        of data on a RAID5 array requires at least two writes to two
        independent drives. The first is the actual block where the data is
        stored and the second is its parity computed from it and all asociated
        blocks on other drives. If the system were to experience any kind of
        failure in between these two writes then the array is left in an
        inconsistent state. The data on the non-parity disks is correct but
        their parity is not. However, when the system detects the issue it
        cannot repair it correctly. When parity is recalculated it will not
        match the one on disk, which is an older version. Alternatively, the
        system can recompute any of the associated blocks it might believe to
        be the issue. That can either be the block written just before the
        failure, resulting in reverting the write, or any of the related
        blocks, resulting in overwriting correct data. Even worse, if any of
        the drives in the array dies, then one of these will happen silently,
        depending on which drive fialed.

        The problem lies in the fact that the system cannot distinguish between
        these three options and the second one leads to data loss. The maths
        works in any of the three configurations, depending on which block we
        choose to be unknown and the hardware will report all of them as
        equally functional (in the general case). What is more, the dataloss is
        completely unrelated to the file, so a filesystem journal cannot hope
        to correct the issue.

        To "close" the write hole, Linux implements two features: a journal for
        RAID \cite{LWN_md_journal} (also called a cache occasionally) and a
        partial parity log (PPL)
        \cite{PPL: https://www.kernel.org/doc/html/latest/driver-api/md/raid5-ppl.html}.
        The journal uses a separate drive to store a journal of writes, much
        like the ZIL and ext4 (\ref{EXT4}, \ref{ZFS}) and then recover any
        corruption on startup. Alternatively, the PPL stores extra (partial)
        parity in the metadata area of parity drives, again allowing for
        recovery on startup.

        Both of theses are imperfect solutions though. The journal requires an
        additional drive (however small) which is very cumbersome for small
        systems like small form factor home servers and laptops (although the
        advent of technologies like M.2 Intel Optane \cite{Optane_homepage} has
        begun alleviating that concern too but it is expensive and an abundance
        of M.2 slots, which tend to be used for boot drives, is not widepsread
        yet). Regardless, the journal is susceptible to the same issues as the
        drives it is trying to protect.  So inevitable errors in the log drive
        (\ref{reliability}) can diminish its benefit.

        The PPL is imperfect too: its documentation \cite{PPL} concedes it
        reduces performance by up to 40\% and in the event of a loss of a
        (dirty) drive its behaviour is the same as unaltered RAID.

    % TODO: incorporate the reliability analysis of ZFS here
    % Does this go here?
    \section{Scrubbing}

        % \cite{https://www.truenas.com/docs/core/tasks/scrubtasks/}
        Drive scrubbing is a necessary feature for both hardware (RAID) and
        filesystem (ZFS) based solutions. It is essentially a regularly
        scheduled automatic full filesystem read that is meant to detect and
        repair silent errors (\ref{reliability}) such that in the event of a
        loss of a drive data is still recoverable and does not cascade into
        more data loss. It is a simple but critical part of reliable data
        storage. The obvious drawback is that this induces significant wear on
        drives, but the benefit greatly outweighs this. If scrubbing is not
        regularly performed, loss of drives can mean data loss is inevitable no
        matter the redundancy solution that is being used. Even a filesystem
        like ZFS with incredible reliability characteristics
        \cite{ZFS_reliability} is vulnerable to this. If scrubbing is omitted
        data corruption may be silent in the case of RAID or more detectible in
        the case of ZFS.

        This technicality was recently demonstrated by a popular tech channel
        \cite{LTT_data_loss}. Due to no scheduled scrubbing and no drive
        failure notifications they experienced major and unrecoverable data
        loss on a storage array of over a petabyte.
