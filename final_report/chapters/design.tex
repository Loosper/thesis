\chapter{Design and implementation}
    \label{sec_design}

    \section{Methodology and design goals}
        \label{sec_methodology}

        % paragraph not proofread (and prob redundant and can be incorporated below)
        The backdrop for the design that follows are two goals: demonstrating
        integrity guarantees in a complete filesystem and maintaining
        simplicity of the design. The main reason behind them is the limited
        time and large potential scope of the project. It is intended that this
        project only be a prototype and not production ready infrastructure.
        This means avoiding special purpose data structures and sticking to the
        most basic solution that gives satisfactory results.

        Designing a filesystem is difficult and implemneting it even more so.
        For example, EXT4's documentation \cite{ext4_docs} is roughly
        \numprint{20000} words of mostly major design features, and around
        \numprint{60000} lines of Linux kernel code. It is not possible to
        achieve such complexity in the time allocated to this project.
        Therefore this filesystem aims to be a proof of concept prototype
        instead. Additionally, designing and implemneting it followed a
        peculiar process to reduce the risk of having an incomplete
        implementation by the submission deadline due to unexpected delays.

        The process boils down a single principle: always have a working
        prototype. The idea is to start with the smallest and simplest
        implementation that works and then add individual features
        incrementally in a manner that always ends up with a working program.
        The plan was to initially complete the large amount of work to have
        \textit{any} filesystem and then incrementally add features until the
        submission deadline. This way regardless of unexpected obstacles and
        complexity underestimation there can be a finished project to present
        and report on, even if this required scaling down the goals.


        % TODO: you can note that the kernel uses a similar thing with their patch
        % process. Think how I developed the arm internship code
        % also add some Agile/Scrum BS from SEP for bonus points
        % maybe say i followed the agile manifesto but not any agile process?
        % instead a kernel process which tends to be similar
        % This incrmeental approach is very close to that of the Agile
        % methodology \cite{agile}.

    \section{FUSE module}
        \label{sec_FUSE}

        Linux Kernel development is a notoriously difficult and time consuming
        process, as evidenced by the many guides on the topic
        \cite{kernelnewbies_developer} \cite{Linux_howto}. Even trivial changes
        to it frequently take days to implement and big projects, such as a
        filesystem, cover a very wide range of topics that \textit{must} be
        considered for the end result to be functional. Things such as
        concurrency, memory limitations, kernel API intricacies, caching and so
        on. For this reason the project is not implemented in the kernel,
        rather in userspace. This avoids the risk of getting bogged down with
        such details and running out of time to implement the project. Instead,
        most of the effort was put towards developing the filesystem in a more
        basic manner.
        % it still is very close tho

        The advantage is that most of these details are either handled in the
        kernel or the standard library. Where they are not, often times they
        can be safely ignored. For example, memory is managed by the simple
        \syscall{malloc} and \syscall{free} in the standard library while
        concurrency can be ignored because by defualt the environment is single
        threaded. For example, the kernel is very particular about its memory
        management and it is preemptive by default. Further, a userspace
        implementation allows the product to be packaged and distributed for a
        much wider selection of kernels. It is widely known that the kernel
        does not maintain a stable API between versions
        \cite{Linux_stable_api}. This means that for compatibility with more
        than one version of the kernel the module has to be rebuilt (and
        potentially edited) upon each release. The alternative to this, of
        course, is upstreaming the module as a driver in the Linux kernel
        itself. However, this is vastly out of scope for this project, ignoring
        the likelyhood of such a major feature being ever accepted. Building
        this project in userspace solves this, as all userspace APIs are
        guaranteed stability \cite{never_break_userspace} and standard
        libraries follow this rule.

        The disadadvantage of this approach is a loss of efficiency. There are
        two factors at play: the kernel-userspace transitions and the lack of
        integration with the kernel. The kernel-userspace transitions are an
        overhead (so much so that there are specific features to minimise their
        occurances such as vDSO \cite{man_vDSO}) that however small is
        measurable and adds up \cite{Linux_context_switch_overhead}). The lack
        of integration is primarily in caching and memory allocation. As a
        consequence, computations will take longer because they will not be
        explicitly cached, instead realying only on what the kernel's caching
        policy might be for arbitrary data, potentially being suboptimal.
        Additionally, binaries will be larger and allocate more memory at
        runtime because they will share less code with the kernel (although
        this will be partially mitigated by shared libraries
        \cite{shared_libraries}) and will use a separate memory address space
        and allocator which prevents optimisations of memory usage with the
        kernel and making memory fragmentation more likely.

        With these in mind, opting for a userspace implementation for this
        project is the best option. This is primarily due to the minimal scale
        and relatively short timescale of the project. This project does not
        need the production readyness of a full in-kernel implementation and
        will only get slowed down. Opting for this approach greatly readuces
        risk as well since development is much more straightforward and less
        error prone.

        The particular mechanism for doing this is the aptly named FUSE
        (Filesystems in USErspace) framework \cite{FUSE}. It provides a kernel
        interface specifically for implementing filesystems as well as a
        userspace library to connect to it with very little code. This is the
        only such interface the Linux kernel provides so it is the only
        recommended option outside of the kernel.

    \section{Supported operations}
        \label{sec_operations}

        As of libfuse version 3.10, there are 44 operations that it exposes to
        filesystems to implement. These include the simple operations like
        \syscall{open}, \syscall{read}, \syscall{write}, etc., but also
        advanced features like asynchronous operations with \syscall{poll},
        extended attribute operations with \syscall{getxattr} or POSIX
        compliance features such as locks with \syscall{getlk}. However, none
        are strictly necessary and may be left out and the FUSE driver will
        return ENOSYS (function not implemented) if an unimplemented system
        call is invoked. Addtionally, some programs will ignore errors with
        particular functions such as \syscall{getxattr} and \syscall{fsync} in
        the case of vim. For this reason a minimal subset that would yield a
        working and usable filesystem of 15 operations is chosen to be
        implemented. These are \syscall{lookup}, \syscall{create},
        \syscall{flush}, \syscall{release}, \syscall{access},
        \syscall{getattr}, \syscall{setattr}, \syscall{open}, \syscall{write},
        \syscall{read}, \syscall{mkdir}, \syscall{unlink}, \syscall{rmdir},
        \syscall{opendir} and \syscall{readdir}. There are the operations that
        almost every program and shell command that interacts with files will
        use. Nice to haves like \syscall{fseek} are done away with as their
        functionality can be obtained via other (even if slightly more verbose)
        means.

    \section{Block size and hardware considerations}
        \label{sec_block_size}
        % \section{Hardware considerations} <- used to be separate
        % \label{sec_hardware}

        In recent years SSDs have become ever more affordable. There has been a
        recent trend for them to replace mechanical hard drives
        \cite{ssd_sales} especially in home computers. With their smaller sizes
        and orders of magnitude greater performance they make perfect sense for
        small form-factor laptops and performant home NAS solutions, which is
        what this project targets (\ref{sec_problem}). Since this is the more
        likely technology for the future (if current trends continue), this
        filesystem targets SSDs instead of classic mechanical hard drives.

        This choice has many consequences (duscussed in \ref{sec_reliability}),
        however one of the more important ones is their performance. Unlike
        hard drives, SSD random access performance is in line with their
        sequentail access capabilities \cite{servethehome_review}. As a result,
        major locality considerations have a lesser impact on performance
        (although they still have some \ref{sec_locality}). Because of this and
        the simplicity goal it makes sense to do away with extents
        (\ref{sec_LFS}, \ref{sec_locality}). They are a common feature for
        increasing throughput but this filesystem does not include them as they
        significantly increase implementation complexity. Other features that
        may necessitate them like tree based space tracking (see
        \ref{sec_free_list}), more involved allocation algorithms like those in
        ext4 (\ref{sec_ext4}) or \citeauthor{ext4_space_maps}'s space tracking
        proposal will not be included so the use of extents is not necessary.
        % The speed advantage of keeping sequentail data sequentailly on disk
        % is important, however, since this filesystem will target SSDs (see
        % \ref{sec_hardware}) the benefit is much less pronounced.

        There is a single provision for increasing data locality - to have a
        minimum allocation size. This can have some impact on performance even
        on SSDs (\ref{sec_locality}) but more importantly reduces the frequency
        of allocations. That minimum is set to 20 blocks for a balance between
        high preallocation but not wasting much space.

        To counteract the drawbacks of lacking extents blocksizes must be
        maximised to reduce bookkeeping overheads. As a sensible default blocks
        are set to 4KiB in size, a minimum dating as far back as FFS
        (\ref{sec_FFS}). It offers a low amount of wasted space in small files
        while maximising performance of sequentail accesses and minimising
        overhead for things like free lists.

    % TODO: proofread again
    \section{Metadata as files}
        \label{sec_files}

        The first design choice, heavily inspired by WAFL (\ref{sec_WAFL}), was
        that all metadata in the filesystem would be kept as a regular file
        instead of having special data structures for each. This principle was
        pedantically followed, unlike filesystems like ext4 (\ref{sec_ext4})
        which leave some metadata in especially formatted places on disk. This
        way implementation complexity is greatly reduced and all metadata can
        benefit from the underlying indexing method. Further, this
        automatically solves the problem of how to allocate space for metadata.
        For example, there is no need make any special consideration for its
        size, placement or alginment. It can begin empty and grow as necessary,
        getting spread throughout the filesystem dynamically depending on
        availability. An added benefit is that this way the inode list can
        efficiently accommodate both of its edge cases. On one hand a
        filesystem with lots of little files can have a large i-list without
        reaching an inode limit. On the other, a directory with a few files
        that take up gigabytes of space will not waste any space towards a
        preallocated (presumably large) i-table.

    \section{The free list}
        \label{sec_free_list}

        A popular way to keep track of available free space is block bitmaps
        where each block in the filesystem has a bit in the free list which
        indicates if the block is free or not. Historically, filesystems have
        frequently used it like FFS \cite{FFS}, WAFL \cite{WAFL} and more
        recently EXT4 \cite{ext4_space_maps} and HFS+ \cite{HFSplus}. They are
        very space efficient, using only 256 KB per GB with 4KB blocks or about
        0.02\%. Their small size allows them to be easily cached and kept up to
        date. A major drawback is that traversing these bitmaps is not
        particularly efficient as it usually devolves into some form of linear
        search, although some techniques can help mitigate this (for example
        storing information about the closest free block).

        There are other techniques available, such as tracking free extents in
        a \bplustree like XFS \cite{XFS_scalability} or a proposed space map
        for EXT4 which relies on RB-trees \cite{ext4_space_maps}. Both of these
        approaches rely on space being managed in extents. Their major drawback is
        the significantly increased complexity.

        In the end a file of the classic bitmap approach was selected. They
        work well enough, their implementation is simple and a variety of
        allocation policies are possible with them. Alternative approaches
        would introduce far too much extra complexity to the project for a
        limited benefit considering the generally basic design. Due to the time
        constraint, time is better spend developing other features.

    % TODO: not proofread
    \section{The inode table and directories}

        Both structures are files containing a linear array of items. At the
        start of each file there is a small header which stores how many
        entries each file has. The only difference is in the items they store.
        The inode table contains inodes while directories store the usual
        (name, i-number) tuples. In both cases this is functionally equivalent
        to UFS (\ref{sec_UFS}).

        Both structures serve very different purposes from a user facing
        perspective. However, the workload they need to complete is very
        similar. Both store a list of unordered entries. The directoy has to be
        searched to find the i-numbers of names. The inode table has to be
        searched to find a free entry to place a new inode. The only functional
        difference is that the inode table can be indexed to produce a result
        in constant time (and to that effect can be considered ordered, however
        the contents themselves have no particular order except their indices).

        As this filesystem strives for simplicity (\ref{sec_methodology}) it
        makes sense to unify the two structures. There exist bespoke
        implementations for both like ext4's Htrees for directories
        (\ref{sec_htree}). However, adding them introduces a lot of extra
        complexity which is undesiriable. Such features will not benefit the
        basic case much as the overwhelming majority of lookup in the inode
        table are i-number to inode resulutions (which are constant time as the
        index is reused in the table). Meanwhile, for home use, which this
        filesystem targets (\ref{sec_problem}), directories tend to not have a
        large number of files in them \cite{contents_strudy}. Therefore an
        imporovement in their speed will not be particularly noticable.

        As a result, a simple linear approach is adopted. It has pretty much
        the same benefits and the same drawbacks as those for the free list as
        the underlying structure is the same.

    \section{inode}

        Once again, to maintain simplicity, the inode contains only essentail
        information. This includes metadata like the owner's user and group
        IDs, the access permissions and creation/modifiaction timestamps.
        Non-metadata fields are the file's size and the head to the tree of
        data nodes, stored as a B-tree (see \ref{sec_design_btree}).

        The metadata is required to make a file understandable to Linux. It is
        also very simple to store as it has a known size and layout (a simple c
        \monospace{struct}), is only stored in one place, and only references
        to it are stored. These values could be hard-coded (for example for
        each file report root as owner with full permissions and the UNIX epoch
        as a creation timestamp) but omitting this information makes the
        filesystem unpleasant to debug, as it can be hard to tell if files with
        the exact same metadata are actually different or there is a bug.

        % TODO: should I include an strace trace of cat?
        The size and data fields are critical to implementing an inode and
        cannot be done away with. The data is obvious, since the file itself
        must be stored somewhere and as the file handle, the inode is the only
        structure that references it. The need for the size is less apparent,
        since all system calls for manipulating a file (\syscall{open},
        \syscall{read}, \syscall{lseek}, \syscall{close} etc), do not reference
        it in any way. The data tree contains enough information for this value
        to calculated whenever needed. For example, a simple post-order
        traversal of the tree (following the rightmost reference at each node)
        will find its last asigned block and therefore its size. However, basic
        programs like \monospace{cat} (which are one of the simplest ways to
        access files \cite{TLDP_proc_access}) request the size of the file
        (with \syscall{stat}) before reading it in full. This presents a
        challenge as, although it is possible to find out this size on each
        such call, it requires a nontrivial amount of code to do so. It was
        deemed to be less code to store this information than to recalculate it
        each time. As an added benefit, this removes a slight inefficiency.

    \section{B-tree}
        \label{sec_design_btree}
        % \label{sec_implementation_btree} % go over these and check cos
        % i combined these two

        Despite its ubiquity, the B-tree is not the only choice for storing
        data blocks for an inode. Simpler techniques have historically been
        used. Originally, UFS and FFS used a generic tree of staggered
        indirection (\ref{sec_UFS} and \ref{sec_FFS}). Another, more well known
        one, was the chain of clusters (effectively a linked list) used by
        FAT32 \cite{fat32}.

        The linked list scheme was experimented with, as it is quite a lot
        simpler than a full B-tree implementation. However, it proved very
        difficult to reason with on random accesses and performance was also
        abysmall (something FAT32 has always struggled with). Because of this
        it was decided to use a \bplustree from an external implementation. The
        B-tree is much more suited for this task (\ref{sec_btree}) and supports
        standard operations like insert, delete and lookup natively and
        performanlty.

        % TODO: move to research?
        The B-tree implementation is perhaps the most important and most
        volumous piece of code in the whole project. For this reason using an
        off the shelf solution was greatly prioritised instead of a custom
        implementation. A problem was that the available ones turned out to be
        unsuitable as they would require major reworks that would creep up in
        scope to a custom implementation. One example is an implementation on
        GitHub \cite{GitHub_btree} but this makes the assumption that the
        B-tree will be in memory and as a consequence uses pointer arithmetic
        across the board. The on-disk representation of the B-tree means the
        logical "pointers" are in fact just block numbers, which are very
        different (albeit numerically identical) to memory addresses. An option
        was to map the disk in memory as a stopgap, however, this presents
        further challenges with making sure \syscall{malloc} and such work in
        that address range and not elsewhere. Another option, SQLite's
        interface \cite{SQLite_btree}, was unsutable as this assumes a database
        context. Other implementations, such as \cite{random_btree} were
        discarded for similar reasons.

        In the end, unsurpisingly, an implementation from the Linux kernel (in
        \monospace{lib/btree.c} \cite{Linux_source}) was chosen. It is a
        general purpose \bplustree implemntation that is fairly freestanding,
        (the bulk of the logic does not depend on the kernel's internals) and
        the few bits that do are abstracted away in a way that is easy to
        change and export (about 30 lines of headers). Further, even though it
        also assumes the tree will be stored in memory, every single access to
        the it is abstracted away in a handful of getter and setter routines
        which exclusively perform any pointer arithmetic. This allowed for the
        whole thing to be stored on disk via those functions without changing
        any internals. Further, despite the code being over a decade old
        (originally submitted in 2009) it has received several bugfixes over
        the years, indicating that it is actively used and maintaned. Adding to
        that, because it is kernel code, it is of extraordinary quality, making
        it a pleasure to work with. There is only one issue that is do with the
        kernel's relatively restrictive GPLv2 licence, which this code is
        licenced under. To resolve this the project adopts the GPLv2 as
        discussed in appendix \ref{app_licence}.

    \section{Data integrity}
        \label{sec_data_integrity}

        The aim is to achieve data integrity with a "RAID with checksums"
        approach. The combination is required as RAID alone cannot be relied
        upon for data integrity (\ref{sec_RAID_problems}) and checksums
        themselves do not provide redundancy.

        First, the checksumming is necessary to combat silent drive
        unreliability (see \ref{sec_reliability}). There are two choices for
        where this can happen: at the data structure level or at the block
        level. If done at the data structure level (i.e. every "pointer" would
        also include a checksum of its data), like EXT4 (\ref{sec_ext4}, then
        there is a benefit that the data and its checksum are spatially distant
        (a provision ZFS has \ref{sec_ZFS}). However, this approach adds
        significant complexity to make sure checks are performed everywhere. If
        done at the block level, then the rest of the filesystem can be
        ignorant about the existance of checksums and carry on as normal,
        reducing the complexity. Since this filesystem does not implement a
        tree of blocks, like ZFS (\ref{sec_ZFS}) does, this has the drawback
        that checksums are local to the data they protect. However,
        implemneting this is very trivial as it only needs to extend the block
        level interface and the entirity of the rest of the design can remain
        the same. This second approach is chosen primarily for this reason.

        For the question of where to store the checksum, there is only one
        solution that is not particularly involved. Storing the checksum within
        the block makes a corruption of the block extremely likely to corrupt
        the checksum too making it a very bad idea (becasue of collisions) plus
        it diminishes the kernel's ability to cache large blocks of well
        aligned data (since the checksum will either eat into the block's
        usable size, wasting cache, or misalign blocks). The next best thing is
        having separate aggregate blocks that contain the checksums for as many
        blocks as possible just after them.

        Next comes the space versus integrity tradeoff of how much space to
        allocate for checksums. On one hand the bigger the checksum the better
        since it reduces the chance for collisions to mess things up. On the
        other hand, we want overhead to be as small as possible so that drives
        can be better utilised. For example, for backwards compatibility, ext4
        allocates 4 bytes for each structure \cite{ext4_docs} and uses a CRC
        algorithm which can only detect a limited number of erronous bits (or
        even correct them in the case of Bose–Chaudhuri–Hocquenghem (BCH) codes
        \cite{flash_error_manual}). ZFS stores 32 bytes \cite{ZFS_docs} for
        each block (usually 128KiB), which is estimated to use around 0.5\%
        \cite{ZFS_overhead} of space.

        % TODO: graphic?
        For the scheme above of collating checksums for blocks in a separate
        block, a 4 byte checksum requires about 0.1\% of space be allocated for
        checksums, 16 bytes are 0.39\% and 32 bytes are 0.78\%. A 0.1\%
        overhead would be ideal, however this limits the choice of checksum
        algorithm severely (to mostly Cyclic Redundancy Checks (CRCs) and
        Fletcher checksums \cite{embedded_checksums}). 64 bytes and more would
        be better for integrity but the overhead reaches single digit
        percentages which becomes significant. 32 bytes is the largest amount
        that is not significant enough to raise eyebrows. To make block numbers
        easy to compute this will mean every 128th block contains the checksums
        for the previous 127. The 128th place in that block remains technically
        unused, so a checksum of the block itself can be added for a bit of
        added integrity.

        % TODO: can reference the locality of SSDs errors?
        Now, since checksums and data are local to each other, the checksum
        algorithm must be as collision resistant as possible to minimise the
        chance a data corruption is left undetected. This is mostly a problem
        on the checksumming blocks as they are the only blocks containing data
        (in the form of checksums) and checksums of the same data on the same
        block.

        % Since we are targetting
        % SSDs (\ref{sec_SSD}) we may align checksum characteristics with SSD
        % failure patterns.
        % TODO: that facebook large scale study idk if cited here. Check i've cited it right
        % TODO: test these scenarios, add checks for them Also, do some mathys
        % analysis of the likelyhood: say 10^-5 of a block corupt, 4 blocks
        % need to corrupt and then a collision too should be like 10^-20 or
        % some shit

        There are many ways to detect errors. Ext4 uses CRC \cite{ext4_docs}
        while ZFS uses sha256 by defualt \cite{ZFS_docs}. Other examples
        include Fletcher or Adler checksums \cite{embedded_checksums} and BCH
        codes \cite{flash_error_manual}. We settle on hash based checksums for
        several reasons. First, we rely on extra drives to provide redundancy
        for error correction (described below) so an error correcting code
        (like BCH) is unnecessary. Then only for detection Adler and Fletcher
        checksums perform worse than good CRCs for a given number of bit errors
        \citeauthor{embedded_checksums}. Finally, the most common CRCs (eg.
        CRC32) produce short check sequences (32 bits for CRC32). A best case
        scenario will have a probability of undetected errors of
        $\frac{1}{2^k}$ where k is the length of the checksum
        \cite{embedded_checksums}. So a longer checksum reduces this
        probability and is better. In any case, CRCs only guarantee detection
        of up to only a handful of bits \cite{embedded_checksums}. While CRC64
        and CRC128 do exist they are not typically implemented in hardware
        meaning they are unlikely to yield good speed. This leaves hashes as
        the most viable option, especially since they strive for similar inputs
        to produce different outputs, which is exactly the use case we expect.

        % TODO: Cite where I got the SHAs from?
        This leaves the choice of hash. Older hashes like MD5 and SHA-1 are
        probably sufficent but they are starting to show their age, for example
        SHA-1 (MD5's successor) is starting to have known collisions
        \cite{SHA_collision}. Besides, we have allocated more space than the
        160 bits necessary for SHA-1 so we can utilise the extra security from
        more modern ciphers. The major candidates are SHA-2 (SHA256) and SHA-3
        (SHA3256). Ideally, we would use the latest and gratest SHA-3, however,
        in the interest of efficiency we select SHA-2. This is because on
        modern architectures it is around 3 times faster \cite{hash_stats} and
        the application we use it for is very unlinkely to experience malicious
        bit manipulation so the security is sufficent.

        Then, for the choice of RAID level, there are many options discussed in
        \ref{sec_RAID}. All have various performance profiles and offer
        protection against various numbers of drive failures. However, for all
        intents and purposes they are interchangable - no matter the level, the
        result is a contiguous pool of redundant storage. So, to once again
        maintain simplicity, RAID 1 (mirroring) is selected. A basic mirror
        provides just enough redundancy while being simple to implement - all
        accesses are identical to both drives. There are no fancy stripe or
        parity calculations. One of the two disks can fail outright and all
        data will still be present. In line with the home user target, two
        drives is the smallest possible redundant number of drives and most
        likely for the reason of cost and space. Any more guarantees are not
        strictly necessary and if required can be added at little cost.

        % TODO: this might really benefit from some maths
        Now, even though data and checksum locality is high, this can be
        compensated for by the RIAD arrangement. In the unlikely event that a
        block and its associated checksum get currupted such that a collision
        occurs and they appear valid, both of these blocks have a carbon copy
        on the other drive. So the filesystem can read the data from one drive
        and its checksum from the other. With the likelyhoods we are working
        with, independent colliding corruptions on both drives is practically
        impossible outside of a malicious attack.

    \section{Bootstrap}

        % TODO: mention that this tried to reduce code duplication too?
        Bootrstrapping a filesystem is usually done by a separate command like
        mkfs \cite{man_mkfs}, leaving the filesystem in an always initialised
        state. For this project the distinction is done away with, opting
        instead for the filesystem to do it itself. This was once again done
        for simplicity, as maintaining a separate binary takes extra makefile
        rules and a more meticulous source code file heirarchy. Nevertheless,
        all of the same funcionality is present and the initialisation routine
        is very clearly separate from the rest of the filesystem so that it can
        still be separated without requiring major code reworks.

    \section{version control and good practices?}
