\chapter{Design}

    \section{Methodology and design goals}
        \label{sec_methodology}

        % paragraph not proofread (and prob redundant and can be incorporated below)
        The backdrop for the design that follows are two goals: demonstrating
        integrity guarantees in a complete filesystem and maintaining
        simplicity of the design. The main reason behind them is the limited
        time and large potential scope of the project. It is intended that this
        project only be a prototype and not production ready infrastructure.
        This means avoiding special purpose data structures and sticking to the
        most basic solution that gives satisfactory results.

        Designing a filesystem is difficult and implemneting it even more so.
        For example, EXT4's documentation \cite{ext4_docs} is roughly
        \numprint{20000} words of mostly major design features, and around
        \numprint{60000} lines of Linux kernel code. It is not possible to
        achieve such complexity in the time allocated to this project.
        Therefore this filesystem aims to be a proof of concept prototype
        instead. Additionally, designing and implemneting it followed a
        peculiar process to reduce the risk of having an incomplete
        implementation by the submission deadline due to unexpected delays.

        The process boils down a single principle: always have a working
        prototype. The idea is to start with the smallest and simplest
        implementation that works and then add individual features
        incrementally in a manner that always ends up with a working program.
        The plan was to initially complete the large amount of work to have
        \textit{any} filesystem and then incrementally add features until the
        submission deadline. This way regardless of unexpected obstacles and
        complexity underestimation there can be a finished project to present
        and report on, even if this required scaling down the goals.


        % TODO: you can note that the kernel uses a similar thing with their patch
        % process. Think how I developed the arm internship code
        % also add some Agile/Scrum BS from SEP for bonus points
        % maybe say i followed the agile manifesto but not any agile process?
        % instead a kernel process which tends to be similar
        % This incrmeental approach is very close to that of the Agile
        % methodology \cite{agile}.

    \section{Block size and hardware considerations}
        \label{sec_block_size}
        % \section{Hardware considerations} <- used to be separate
        % \label{sec_hardware}

        In recent years SSDs have become ever more affordable. There has been a
        recent trend for them to replace mechanical hard drives
        \cite{ssd_sales} especially in home computers. With their smaller sizes
        and orders of magnitude greater performance they make perfect sense for
        small form-factor laptops and performant home NAS solutions, which is
        what this project targets (\ref{sec_problem}). Since this is the more
        likely technology for the future (if current trends continue), this
        filesystem targets SSDs instead of classic mechanical hard drives.

        This choice has many consequences (duscussed in \ref{sec_reliability}),
        however one of the more important ones is their performance. Unlike
        hard drives, SSD random access performance is in line with their
        sequentail access capabilities \cite{servethehome_review}. As a result,
        major locality considerations have a lesser impact on performance
        (although they still have some \ref{sec_locality}). Because of this and
        the simplicity goal it makes sense to do away with extents
        (\ref{sec_LFS}, \ref{sec_locality}). They are a common feature for
        increasing throughput but this filesystem does not include them as they
        significantly increase implementation complexity. Other features that
        may necessitate them like tree based space tracking (see
        \ref{sec_free_list}), more involved allocation algorithms like those in
        ext4 (\ref{sec_ext4}) or \citeauthor{ext4_space_maps}'s space tracking
        proposal will not be included so the use of extents is not necessary.
        % The speed advantage of keeping sequentail data sequentailly on disk
        % is important, however, since this filesystem will target SSDs (see
        % \ref{sec_hardware}) the benefit is much less pronounced.

        There is a single provision for increasing data locality - to have a
        minimum allocation size. This can have some impact on performance even
        on SSDs (\ref{sec_locality}) but more importantly reduces the frequency
        of allocations. That minimum is set to 20 blocks for a balance between
        high preallocation but not wasting much space.

        To counteract the drawbacks of lacking extents blocksizes must be
        maximised to reduce bookkeeping overheads. As a sensible default blocks
        are set to 4KiB in size, a minimum dating as far back as FFS
        (\ref{sec_FFS}). It offers a low amount of wasted space in small files
        while maximising performance of sequentail accesses and minimising
        overhead for things like free lists.

    % TODO: proofread again
    \section{Metadata as files}
        \label{sec_files}

        The first design choice, heavily inspired by WAFL (\ref{sec_WAFL}), was
        that all metadata in the filesystem would be kept as a regular file
        instead of having special data structures for each. This principle was
        pedantically followed, unlike filesystems like ext4 (\ref{sec_ext4})
        which leave some metadata in especially formatted places on disk. This
        way implementation complexity is greatly reduced and all metadata can
        benefit from the underlying indexing method. Further, this
        automatically solves the problem of how to allocate space for metadata.
        For example, there is no need make any special consideration for its
        size, placement or alginment. It can begin empty and grow as necessary,
        getting spread throughout the filesystem dynamically depending on
        availability. An added benefit is that this way the inode list can
        efficiently accommodate both of its edge cases. On one hand a
        filesystem with lots of little files can have a large i-list without
        reaching an inode limit. On the other, a directory with a few files
        that take up gigabytes of space will not waste any space towards a
        preallocated (presumably large) i-table.

    \section{The free list}
        \label{sec_free_list}

        A popular way to keep track of available free space is block bitmaps
        where each block in the filesystem has a bit in the free list which
        indicates if the block is free or not. Historically, filesystems have
        frequently used it like FFS \cite{FFS}, WAFL \cite{WAFL} and more
        recently EXT4 \cite{ext4_space_maps} and HFS+ \cite{HFSplus}. They are
        very space efficient, using only 256 KB per GB with 4KB blocks or about
        0.02\%. Their small size allows them to be easily cached and kept up to
        date. A major drawback is that traversing these bitmaps is not
        particularly efficient as it usually devolves into some form of linear
        search, although some techniques can help mitigate this (for example
        storing information about the closest free block).

        There are other techniques available, such as tracking free extents in
        a \bplustree like XFS \cite{XFS_scalability} or a proposed space map
        for EXT4 which relies on RB-trees \cite{ext4_space_maps}. Both of these
        approaches rely on space being managed in extents. Their major drawback is
        the significantly increased complexity.

        In the end a file of the classic bitmap approach was selected. They
        work well enough, their implementation is simple and a variety of
        allocation policies are possible with them. Alternative approaches
        would introduce far too much extra complexity to the project for a
        limited benefit considering the generally basic design. Due to the time
        constraint, time is better spend developing other features.

    % TODO: not proofread
    \section{The inode table and directories}

        Both structures are files containing a linear array of items. At the
        start of each file there is a small header which stores how many
        entries each file has. The only difference is in the items they store.
        The inode table contains inodes while directories store the usual
        (name, i-number) tuples. In both cases this is functionally equivalent
        to UFS (\ref{sec_UFS}).

        Both structures serve very different purposes from a user facing
        perspective. However, the workload they need to complete is very
        similar. Both store a list of unordered entries. The directoy has to be
        searched to find the i-numbers of names. The inode table has to be
        searched to find a free entry to place a new inode. The only functional
        difference is that the inode table can be indexed to produce a result
        in constant time (and to that effect can be considered ordered, however
        the contents themselves have no particular order except their indices).

        As this filesystem strives for simplicity (\ref{sec_methodology}) it
        makes sense to unify the two structures. There exist bespoke
        implementations for both like ext4's Htrees for directories
        (\ref{sec_htree}). However, adding them introduces a lot of extra
        complexity which is undesiriable. Such features will not benefit the
        basic case much as the overwhelming majority of lookup in the inode
        table are i-number to inode resulutions (which are constant time as the
        index is reused in the table). Meanwhile, for home use, which this
        filesystem targets (\ref{sec_problem}), directories tend to not have a
        large number of files in them \cite{contents_strudy}. Therefore an
        imporovement in their speed will not be particularly noticable.

        As a result, a simple linear approach is adopted. It has pretty much
        the same benefits and the same drawbacks as those for the free list as
        the underlying structure is the same.

    \section{inode}

        Once again, to maintain simplicity, the inode contains only essentail
        information. This includes metadata like the owner's user and group
        IDs, the access permissions and creation/modifiaction timestamps.
        Non-metadata fields are the file's size and the head to the tree of
        data nodes, stored as a B-tree (see \ref{sec_design_btree}).

        The metadata is required to make a file understandable to Linux. It is
        also very simple to store as it has a known size and layout (a simple c
        \monospace{struct}), is only stored in one place, and only references
        to it are stored. These values could be hard-coded (for example for
        each file report root as owner with full permissions and the UNIX epoch
        as a creation timestamp) but omitting this information makes the
        filesystem unpleasant to debug, as it can be hard to tell if files with
        the exact same metadata are actually different or there is a bug.

        % TODO: should I include an strace trace of cat?
        The size and data fields are critical to implementing an inode and
        cannot be done away with. The data is obvious, since the file itself
        must be stored somewhere and as the file handle, the inode is the only
        structure that references it. The need for the size is less apparent,
        since all system calls for manipulating a file (\syscall{open},
        \syscall{read}, \syscall{lseek}, \syscall{close} etc), do not reference
        it in any way. The data tree contains enough information for this value
        to calculated whenever needed. For example, a simple post-order
        traversal of the tree (following the rightmost reference at each node)
        will find its last asigned block and therefore its size. However, basic
        programs like \monospace{cat} (which are one of the simplest ways to
        access files \cite{TLDP_proc_access}) request the size of the file
        (with \syscall{stat}) before reading it in full. This presents a
        challenge as, although it is possible to find out this size on each
        such call, it requires a nontrivial amount of code to do so. It was
        deemed to be less code to store this information than to recalculate it
        each time. As an added benefit, this removes a slight inefficiency.

    \section{B-tree}
        \label{sec_design_btree}

        Despite its ubiquity, the B-tree is not the only choice for storing
        data blocks for an inode. Simpler techniques have historically been
        used. Originally, UFS and FFS used a generic tree of staggered
        indirection (\ref{sec_UFS} and \ref{sec_FFS}). Another, more well known
        one, was the chain of clusters (effectively a linked list) used by
        FAT32 \cite{fat32}.

        The linked list scheme was experimented with, as it is quite a lot
        simpler than a full B-tree implementation. However, it proved very
        difficult to reason with on random accesses and performance was also
        abysmall (something FAT32 has always struggled with). Because of this
        it was decided to use a \bplustree from an external implementation. The
        B-tree is much more suited for this task (\ref{sec_btree}) and supports
        standard operations like insert, delete and lookup natively and
        performanlty.

    \section{Redundancy}

        The aim for the redundancy is "RAID with checksums", since RAID alone
        cannot be relied upon for data integrity (\ref{sec_RAID_problems}).

        First, to combat silent drive unreliability (see \ref{sec_reliability})
        all data must be checksummed. There are two choices for where this can
        happen: at the data structure level or at the block level. If done at
        the data structure level (i.e. every "pointer" would also include a
        checksum of its data), like EXT4 (\ref{sec_EXT4}, then there is a
        benefit that the data and its checksum are spatially separate (a
        provision ZFS has \ref{sec_ZFS}). However, this approach adds
        significant complexity for checks at all places. If done at the block
        level, then the rest of the filesystem can be ignorant about the
        existance of checksums and carry on as normal, reducing complexity and
        size requirements. Since this filesystem does not implement a tree of
        blocks, like ZFS (\ref{sec_ZFS}) does, this has the drawback that
        checksums are local to the data they protect. However, implemneting
        this is very trivial as it only needs to extend the block level
        interface and the entirity of the rest of the design can remain the
        same. This second approach is chosen for primarily this reason.

        For the question of where to store the checksum, there is only one
        solution that is not particularly involved. Storing the checksum within
        the block makes a corruption of the block extremely likely to corrupt
        the checksum too making it a very bad idea. The next best thing is
        having separate aggregate blocks that contain the checksums for as many
        blocks as possible just after them.

        Next comes the question of how much space to allocate for checksums.
        This is a space versus integrity tradeoff. On one hand the bigger the
        checksum the better since it allows for a lesser chance for collisions
        to mess things up. On the other hand, we want overhead to be as small
        as possible so that drives can be better utilised. For example, ext4
        allocates 4 bytes for each structure \cite{ext4_docs} but that is
        mostly for backwards compatibility and there simply not being room for
        more. ZFS stores 32 \cite{ZFS_docs}[basic\_cocepts/chechsums] for each
        block (usually 128KiB), which is estimated to be around 0.5\%
        \cite{ZFS_overhead}.

        For the scheme above of collating checksums for blocks in a separate
        block, a 4 byte checksum requires about 0.1\%, 16 bytes are 0.39\% and
        32 bytes are 0.78\%. A 0.1\% overhead would be idea, however this
        limits the choice of checksum algorithm severely (mostly Cyclic
        Redundancy Checks (CRCs) and Fletcher checksums
        \cite{embedded_checksums}). 64 bytes and more would be better for
        integrity but the overhead reaches single digit percent overhead which
        becomes significant. 32 bytes is the largest amount that is not
        significant enough to raise eyebrows. To make block numbers easy to
        compute this will mean every 128th block contains the checksums for the
        previous 127. The 128th place will be technically unused, so a checksum
        of the block itself for a bit of added integrity.

        Now, since checksums and data will be local to each other, the checksum
        algorithm must be as resiliant as possible to minimise the chance that
        a data corruption results in a checksum collision and the data is
        reported as unaltered. Even though the block checksums itself and that
        checksum is stored on the same block next to all the others, a corrupt
        block could feasibly alter an arbitrary checksum and the metachecksum
        at the same time. Since we are targetting SSDs (\ref{sec_SSD}) we may
        align checksum characteristics with SSD failure patterns.

        One way to measure SSD failures is with the raw bit error rate (RBER).
        The RBER is defined as the number of currupted bits per the number of
        total bits read \cite{flash_reliability}.
        \citeauthor{flash_reliability} take measurements over a vast number of
        drives with single-level cell (SLC, 1 bit per memory cell) and
        multi-level cells (MLC, 2 bits). They measure RBERs between $10^{-8}$
        and $10^{-5}$. \citeauthor{bit_error_mlc} finds similar rates for MLC.
        On the other hand \citeauthor{bit_error_qlc} finds that triple-level
        cells (TLC) and quad-level cell (QLC) suffer RBER rates of around
        $10^{-3}$ and $10^{-2}$ respectively. Although
        \citeauthor{flash_reliability} also note that RBER rates are a poor
        indicator of unrecoverable errors it puts an upper bound on their
        likelyhood. Also all of them (\cite{flash_reliability},
        \cite{bit_error_mlc}, \cite{bit_error_qlc}) cite error correcting codes
        to mitigate these errors we can expect much lower rates that would
        trigger our checksum recovery mechanisms.

        % TODO: that facebook large scale study idk if cited here. Check i've cited it right

        From the above we can expect a worst case failure of rate of a few bits
        per block. As \citeauthor{flash_reliability} and
        \citeauthor{flash_large_scale} note, these failures are not uniform and
        change with SSD age and wear. With further analysis from
        \citeauthor{flash_error_manual} it is apparent that this is due to a
        wide range of physical phenomena. However, it becomes apparent that
        these errors are relatively isolated from each other where only
        adjacent cells can affect each other due to the way cells store
        electrons. Further, due to effective error correcting techniques actual
        unrecoverable errors are much lower than the RBER.

        There are many ways to detect errors. Ext4 uses CRC \cite{ext4_docs}[checksums]
        while ZFS uses sha256 \cite{ZFS_docs}[checksum]. Other examples include
        Fletcher, Adler checksums \cite{embedded_checksums} and
        Bose–Chaudhuri–Hocquenghem (BCH) \cite{flash_error_manual}. We settle
        on hash based checksums for several reasons. First, we rely on extra
        drives to provide redundancy for error correction (to be described
        next) so a BCH is unnecessary. Then Adler and Fletcher checksums
        perform worse than good CRCs for a given number of bit errors
        \citeauthor{embedded_checksums}. Finally, the most common CRCs (eg.
        CRC32) produce short check sequences (32 bits for CRC32). A best case
        scenario will have a probability of undetected errors of
        $\frac{1}{2^k}$
        % BAD sentence (bad until end of paragraph tbh)
        where k is the length of the checksum \cite{embedded_checksums} and a
        longer checksum simply yields a lower probability. While CRC64 and
        CRC128 do exist they are not typically implemented in hardware so are
        unlikely to yield good speed. This leaves hashes as the most viable
        option, especially since they strive for similar inputs to produce
        different outputs, which is exactly the use case we expect.

        % Cite where I got the SHAs from?
        This leaves the choice of hash. Older hashes like MD5 and SHA-1 are
        probably sufficent but they are starting to show their age, for example
        SHA-1 is starting to have known collisions \cite{SHA_collision}.
        Besides, we have allocated more space than the 160 bits for SHA-1 so we
        can use the extra security from more modern ciphers. The major
        candidates are SHA-2 (SHA256) and SHA-3 (SHA3256). Ideally, we would
        use the latest and gratest SHA-3, however in the interest of efficiency
        we select SHA-2. This is because on modern architectures it is around 3
        times faster \cite{hash_stats} and the application we use it for is
        very unlinkely to experience malicious bit manipulation so the security
        is sufficent.

        % TODO: have the checksum be outside the block. Modify the allocator
        % somehow. OOOOO you can even checksum the checksum blocks too (might
        % be a good idea as it might not be too much memory?)

        Then, for the choice of RAID level, there are many options, from the
        classic RAID levels 1 thorough 6, to some nonstandard and exotic
        versions like RAID-Z (\ref{sec_RAID}). All have various performance
        profiles and offer protection against various number of drive failures
        However, for all intents and purposes they are interchangable - no
        matter the level, the result is a contiguous pool of redundant storage.
        For this reason, as a proof of concept, RAID 1 is selected. A basic
        mirror provides just enough redundancy while being simple to implement.
        There are no fancy stripe or parity calculations. One of the two disks
        can fail outright and all data will still be intact.  In line with the
        home user target, two drives is the smallest possible redundant number
        of drives and most likely for the reason of cost and space. Any more
        guarantees are not strictly necessary and if required can be added at
        little cost.

        % TODO: test these scenarios, add checks for them Also, do some mathys
        % analysis of the likelyhood: say 10^-5 of a block corupt, 4 blocks
        % need to corrupt and then a collision too should be like 10^-20 or
        % some shit
        Now, even though data and checksums are relatively local, this is
        compensated for by the RIAD arrangement. In the unlikely event that a
        block gets currupted and its associated checksum block gets corrupted
        too in such a way as to cause a collision and appear to be correct,
        both of these blocks have a mirrored copy. Their checksums can be
        compared to see if they are identical. The odds of two independent
        blocks corrupting in such a way as to cause a hash collision and doing
        so twice on two independent drives is extremely low.
