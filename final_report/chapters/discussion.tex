\chapter{Discussion and Conclusion}

    % \section{the perfomance} or design?
    \section{the implementation}

        Modern filesystems' complexity is very high and the performance
        analaysys (\ref{sec_benchmark} and \ref{sec_perf}) show that it is
        justified. Each seemingly small addition has a large impact on some
        workload. Provided enough such features are included, most workloads
        are efficient. There is no silver bullet feature that magically makes a
        filesystem fast. This is why the performance of the proposed filesystem
        can never match or exceed that of any established filesystem. However,
        there are three features that cover a wide range of workloads and can
        bring performance most of the way there.

        First is the use of B-trees. As we have already seen (\ref{sec_btree})
        they are well established and the sequentail read performance of this
        filesystem shows it is competitive.

        Second is the use of extents, which have been included as far back as
        \citeyear{FFS}. This filesystem does not include them under the
        assumption that SSDs can handle lots of random accesses well. While
        this remains true, not collating chunks of storage introduces lots of
        additional metadata the processing of which amplifies the number of
        accesses required exponentially. This is most apparent (but not
        limited) to allocations and sequential accesses because individually
        addressing each data block for workloads measured in the giga and tera
        bytes requires millions of entreis per gigabyte of data (as noted in
        \ref{sec_perf}) instead of the handful that might be needed if extents
        are present. Even if we do not consider the space requirement for the
        metadata itself, no matter how fast as a random access may be, having
        more of them reduces observed efficiency. A logarithmic complexity of
        any underlying data structure (like the B-tree) is dwarfed when a
        linear or a large polynomial of accesses are required.

        The original assumptions made in \ref{sec_block_size} remain true,
        however, they fail to take into account the above overheads unrelated
        to drive speed. Due to computers historically using mechanical hard
        drives, the literature focues on the drawback of random accesses and
        the metadata amplifcation issue is not apparent.

        The third and most subtle feature is a polished implementation.  to
        integrate all features. While this is not a feature as such, it allows
        for all other features to be utilised properly. Due to the focus on
        data integrity and the experimental nature of this project, this was
        not emphasised and performance suffers accordingly. This prototype
        successfully demonstrates data integrity however as a prototype is has
        not production readiness.

        With this in mind, the myriad of features and tens of thousands of
        lines of code in ext4 (\ref{sec_ext4}) fall into place. It is a huge
        project developed by many people over a very long time. A modern
        performant filesystem cannot be simple by design nor by implementation
        and a simple filesystem cannot be performant. The times of UFS
        (\ref{sec_UFS}) are long gone and modern computer systems are
        inherently complex.

    \section{Did this solve the problem? (Outcome)}
    % more like the reliability

        % omits things like the chaining of checksums
        Despite the performance of this filesystem, its main task - data
        reliability - performs well. It is not ironclad, like ZFS, as it does
        not implement every possible feature and does not actively protect
        against every eventuality. It is, however, also not meant to be.
        Covering every risk under the sun introduces extra complexity and
        configuration considerations which are simply overkill in a home
        setting. This is especially true as sensitive data should have
        additional data regardless of redundancy \cite{Backblaze_321}. Getting
        a robust implementation that gets most of the way is sufficent. This
        filesystem does exactly that. Having data integrity guarantees is many
        orders of magnitude better than current filesystems' reliance on the
        questionable track record of SSDs, if for no other reason than peace of
        mind. It also provides the safety of RAID without a lengthy
        cost-benefit analysis of different techniques to mitigate the various
        problems and the drawbacks that might come with those mitifations
        (\ref{sec_RAID_problems}).

        For laptops this filesystem is of limited use. The trend towards low
        profile and minimalistic devices makes adding an extra drive cost and
        space prohibitive. Nevertheless, the checksumming can still be utilised
        as a data integrity guarantee without any redundancy. New developments,
        like Apple's M1 Ultra chips with on-board flash controllers
        \cite{m1_ultra_controller} can make this setup viable with a single
        flash board although this is an area for further research.

        For home and office desktop computers this filesystem makes a lot of
        sense. These machines are bulky enough to house several drives and
        accommodate any redundancy setup that may be desired. The lower
        administion overhead can help otherwise sceptical users to the
        complexity of solutions like ZFS (\ref{sec_ZFS}) make the leap add some
        much needed integrity to their storage. Especially for office use the
        extra integrity can be useful in mission critical environments where
        corrupted files can cost many times more than the hardware requirement
        to prevent them.

        There has been a recent trend towards cloud based backup solutions like
        Google's Drive \cite{gdrive}, Apple's iCloud \cite{icloud} or ad-hoc
        services like Backblaze \cite{Backblaze_321} or Dropbox \cite{dropbox}
        instead of relying on any redundancies on the devices themselves. These
        solutions perform very similar integrity checks, hashing all files, and
        regularly uploading all specified directories to the cloud where full
        integrity and redundancy is guaranteed \cite{Backblaze_arch} with much
        higher degrees of security and usually also preserve file history.
        These solutions tend to have a low but monthly price (usually around
        Â£8 for 2TB).

        These are very good backup solutions, however their are of limited use
        as simple redundancy. Replacing misbehaving or outright failing drives
        may be isolated from data loss, however, it will certainly involve long
        down time. When it comes to pure integrity they are valid solutions as
        they integrate well with operating systems and the history features
        allow for recovery of silent data corruption. They are slightly
        inferior, however, as silent data corruption may have to be manually
        corrected even if all data is preserved. One major drawback though is
        the long-term price. The low monthly costs are unlikely to raise
        eyebrows but over longer periods they add up to a similar amounts of
        owning the drives. As a result, it is believed that these solutions are
        viable alternates but not direct competitiors to the proposed
        filesystem with user preference being the biggest factor.

    % TODO: you need more discussion. It's 40% of grade and it's like 6 pages
    % \section{Choice of userspace?}

    \section{Unexpected benefits of the methodology}

        The choice of methodology (\ref{sec_methodology}) greatly benefited
        this project. The aim for an always working prototype lead to many
        issues being timely discovered and desgin altered before too much
        momentum had been built up. There were two notable examples for this:
        the everything is a file phylosophy (\ref{sec_files}) and the linked
        list method for storing file data (\ref{sec_design_btree}).

        Storing all metadata as a file presented an interesting circular
        problem: the routine to write a file depended on a routine to extend a
        file when writing past its end. However, to allocate space for a file
        the free list (another file) needed to be written by the same routine.
        Small bugs frequently resulted in infinite recursion which is slow to
        debug. Identifying this flaw early allowed for design to be modified to
        reduce the problem. Some other timely changes were keeping the free
        list out of the inode table, making its access direct instead of
        thorugh a level of recursive indirection. Another change was placing
        the block number on which an inode is stored in the inode itself to
        allow for updating its size to happen directly.

        The linked list method of storing data blocks was particulary
        troublesome. Despite an inital expectaion for simplicity, reasoning
        with its block layout and performing calculations was very difficult
        and error prone. Seemingly simple operations like calculating which
        sequential block an arbitrary byte would reside in was a big routine
        with small edge cases that lead to duplicate code. Luckily, this
        prompted this approach to be abandoned and replaced with a B-tree.
        Perhaps largely owing to its complete implementation borrowed from
        Linux, the project's complexity dropped substantially and progress was
        much quicker.

        Had a more upfront method been used for this project (predefined
        targets for example) it would have failed. Its high complexity and wide
        field of factors for considerations meant any preset plan could crumble
        at any moment. This way a general goal was set and the project
        naturally evolved to what it is now.

    \section{Conclusions}

    \section{Ideas for future work}

        This project is far from a production ready filesystem. There are many
        features that have been left out and some things that can be considered
        as they may be suitable for this project.

        The most apparent one is full coverage of all operations that FUSE
        exposes. For a truly general purpose and user friendly implementation,
        it must support any use case thrown at it.

        Then, in the interest of time, some code is not in a desirable state.
        Examples include some duplication between the directory and inode table
        code, block "pointers" use a generic type where they should ideally
        have one of their own even if just for semantics and general
        optimisation improvements as fixed data structures are repeatedly read
        where this isn't strictly necessary. Memory management could also be
        much better to reduce the programs' memory footprint. While on the
        topic of code, the B-tree implementation's use of pointer has been
        hijacked to carry non-memory related values. Converting the types to a
        new descriptive one would be a great improvement to the code.

        ZFS's tree of blocks (\ref{sec_ZFS}) is also a prime candidate for
        inclusion. The current impelementation has checksums of all blocks,
        however it does not have checksums of checksums. ZFS does this in a
        manner very similar to a blockchain and is a very elegant solution to
        the integrity problem. It has some performance implications, but
        adapting it here may lead to big integrity guarantee improvements.

        Notable omissions from the project are concurrent access and caching
        support. Finding a way to include them would greatly improve
        performance and is practically a must for a real filesystem.

        Reliability detection - too many errors will cause the drive to be expunged

        the performance lol
