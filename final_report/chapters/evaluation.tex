\chapter{Evaluation and results}

    \section{Benchmarking}
        \label{sec_benchmark}

        % the metdata benchmark is an IO benchmark. Argue this.

        Benchmarking a file system involves many aspects and different authors
        have done it in varying ways over the years.  \citeauthor{FFS} measures
        raw throughput of data, \citeauthor{LFS} recorgnise both small reads
        and writes as well as long sustained accesses.
        \citeauthor{soft_updates} on the other hands looks at speed of metadata
        access and modificaiton while \citeauthor{ext4_space_maps} uses a small
        file benchmark that simulates and email server. Finally, Google use a
        hybrid approach where they try a large variety of real world
        representative data with a large distribution of sizes. In the mean
        time popular consumer review sources add random accesses to the mix
        \cite{servethehome_review}, IO operations per second (IOPS)
        \cite{tomshardware_review} and many other ways of looking at
        performance of drives. These last methods are a bit different in that
        they measure drive performance without usually varying the filesystem
        but they are still representative as they aim to find bottlenecks and
        measure the performance of likely usecases.

        Despite the variety of test methodologies the important metrics for
        filesystems and SSDs become apparent: the raw read and write throughput
        and the number of individual IO requests they can fulfill. This is
        measured in the extremes and in various combinations of the two to
        represent different use cases like metadata access, office work or
        fileserver workloads.

        To measure these a utility written by the author of the new Linux
        system call interface io\_uring, \citeauthor{IO_uring}, is used - fio
        \cite{fio}. Using fio a test is devised to test the critical points
        that the above test for: random IO, sequential IO and metadata
        modificaiton. This is implemented as 8 'jobs' - individual tasks that
        fio completes. A Read, a write and a combined test for both small (4K
        size) and large (1M size) files, for a total of 6.  Then a sequential
        read and write test of 1GB of data and finally creating and removing
        100 files in a loop. Each of these tests is perfomerd in sequence for
        20 seconds each.

        All tests are perfomerd using the same hardware and sofware setup: a
        loopback device is set up using a regular file on the filesystem as a
        backing store. The drive is a "SAMSUNG MZVLB1T0HBLR-000L7" high speed
        NVMe drive tested on an AMD Ryzen 7 PRO 4750U CPU. The test was ran 3
        times and average values were taken. As controls two more filesystems
        were tested: ext4 and ntfs (with the ntfs-3g driver). They represent
        two of the most modern filesystems used today and are the main
        competitors to the this project (as described in section \ref{???; idk
        if i say it there}. All tests were performed by completely wiping each
        drive before commencing for consistency.

    \section{Test results}

        The results of the benchmark above are in figure \ref{fig_benchmark}.
        Combined read and write tests have the read and write components added
        toghether in the summary. The raw data for the graph can be found in
        appendinx \ref{app_benchmark}.

        % \begin{wrapfigure}{L}{0.2\textwidth}
        % \end{wrapfigure}
        \begin{figure}[h!]
            \caption{Benchmark results}
            \input{./figures/benchmark}
            \label{fig_benchmark}
        \end{figure}

        The ext4 and ntfs controls appear to behave as expected. Their
        performance is comparable although ntfs performs worse across the board
        as it is natively implemented on Windows and the Linux driver has
        historically been lagging behind. Regardless, the ext4 results are a
        best case maximum whereas the ntfs ones are a more realistic
        representation of an in-kernel but imperfect filesystem.

        The proposed filesystem's performs an order of magnitude lower across
        the board. As discussed in the design stage (\ref{sec_design})
        performance was expected to be worse. Random reads and writes are of
        comparable speed. This is not surprising, as the code paths they take
        are similar. Each access needs to have its destination resolved and
        then data written in the same way. Interestingly, however, sequential
        reads are about an order of magnitude higher than the overall
        perfomance of the filesystems and the corresponding sequential writes.
        This cannot be attributed to drive assymetry, as the difference tends
        to be linear in SSDs \cite{servethehome_review}. Ext4 handles both
        workloads identially, indicating that is not the case. The ntfs-3g
        driver is a reverse engineering effort of the proprietary ntfs
        filesystem and has been notorious for having a problematic
        implementation. To confirm this, we perform profiling analysys in
        \ref{sec_perf}.

        The large file access is in line with the combined sequential test
        indicating that further increases of file size do not reduce
        performance. Finally, the metadata benchmark is surprising as it is in
        line with ext4 and ntfs. This is tahnks to directory and inode files
        having a count at the start of the file so finding a free slot is a
        very fast operation.

    \section{Profiling analysys}
        \label{sec_perf}

        To investigate why the performance is what it is the filesystem was
        profiled with perf. Perf is powerful Linux tool (part of the kernel)
        for program profiling. It can produce performance statistics for
        hardware (cache hits, idle CPU cycles etc) and for sofware (call
        graphs, call frequency, work in each subroutine etc.) \ref{perf}.

        A single profile was taken with \monospace{perf record -g
        <filesystem>}. For a workload, a full run of the benchmark was used
        \ref{sec_benchmark}. Then resulting 147Mb profile was analysed with
        \monospace{perf report -n --children} and \monospace{perf report -n
        --no-children}.

        Unsurpsiginly, most of the time is spent in the kernel (upwards of
        90\%). However, looking into which subroutine enters the kernel (with
        \monospace{--children}) has interesting results. The biggest time
        consumer is, as expected the file access subroutine
        (\monospace{do_read_write_full()}).  However, in it the block
        read/write subroutines (\monospace{read_block()} and
        \monospace{write_block()}) do not make up even a quarter of the time
        spent. Instead, on overwhelming majority of the time is spent in btree
        accesses (\monospace{btree_lookup64()} in this case). Upon inspecting
        the code this is due to two reasons: First, each block is located
        independently of all others and the sequential nature of the \bplustree
        is not utilised (see \ref{sec_btree} design or impl?). Second, large
        writes are handled like a sequence of small writes (with
        \monospace{do_read_write_block()}). Even if sequences of blocks could
        be utilised they could not be observed by the read/write subroutine as
        it never receives them. As virtually all accesses in this filesystem
        relate to a file (\ref{sec_files}), which are accessed with these
        subroutines, all workloads behave identially porrly.

        Knowing this, to explain the sequential read anomally is not hard. With
        this arrangement reading any file will always be faster as it never
        needs to perform file expansion which requires allocation for a small
        speedup as this is un infrequent occurence. It will also, however,
        bypass the file lookup to find the inode itself to begin the read. In
        effect this removes a level of (inefficient) indirection recovering
        about half of the lost performance due to the poor use of the btrees.
        The sequential writes perform poorly because they make up the lost
        level of indirection by expanding the file in a similar manner,
        degrading performance about the same amount that was gained.

        From this analaysys there arises a corollary: allocation cannot be
        fast. And in fact this is the reason why even the largest test is
        relatively small for today's standards. Since all reads and writes are
        treated as individual requests with the size of a block, allocating a
        gigabyte of space requires about 268 million calls to the allocation
        subroutine. Since it is also slow, the results are untenable.

        Benefit: simple impl. The kernel has all of that so it's fast.

% actual percents. TODO: put in listing?
% -   93.79%     0.01%            36  filesystem   filesystem            [.] do_read_write_full
%    - 93.78% do_read_write_full
%       - 93.55% do_read_write_block
%          - 86.10% get_pblock_of_byte
%             + 86.10% btree_lookup64
%          + 6.44% file_add_space
%          + 0.61% __libc_pread

        TODO: full perf availble in???
        say implementation is bad, how its bad but don't suggest anything else. That's a discussion


    \section{Failure simulation}

        To simulate failures we use a similar approach to the benchmarking.
        Once again, we use fio. This time a 5 minute stress test is perfomerd.
        It has two jobs which run concurrently. The first one preforms the same
        1GB combined read and write load while the second one creates and
        removes 100 files repeatedly. While this test is running a script goes
        over the first drive in the array and flips a random number of bits on
        each block up to a maximum of 100 in an infinite loop.

        As expected, the 5 minute stress test runs without any user visible
        errors. The filesystem detects and corrects all corrupt blocks and
        returns correct data stored on the redundant drives.

        % TODO: add a figure with the debug output that the error was detected and fixed

        probabilistic, not a guarantee like ZFS. ZFS is a "blockchain" I just
        minimise the likelyhood by many orders of magnitude

    \section{Comparison to existing alternatives}
