\documentclass[a4paper]{report}
\usepackage[a4paper, portrait, left=2.25cm, right=2.25cm, top=2cm, bottom=2cm]{geometry}
\usepackage{hyperref}
\begin{document}


\chapter{things I know}
    SCSI - an old standard, that is obsolete by itself. The current hard drive standards (SATA/SAS) rely on it to communicate.

    Fast filesystem - made a bitmap of free blocks

    Soft update - used for small file writes

    overwriting vs COW fs seems to be a major topic

    blocksize: seems to be 4k fragments, 32k actual block size (at least on BSD, what does ext do?); used to be less
    physical: 4k, but used to be 512B for the longest time

    would be nice to have a checksum for everything (hard on an overwriting fs, since stuff is changing)
    apparently current FSs do it well!

    freeBSD uses UFS2, which is a contemporary of the original UFS (and this is the default)
    netflix uses UFS2, apparently stated here: \url{https://www.youtube.com/watch?v=veQwkG0WdN8}
\section{filesystems}
    UFS -> FFS
    \subsection{UFS}
        1. Lion's book has an incredible explanation of what the directory graph
        looks like, also about when linking/unlinking does what
        2. Ritchie/Thompson UNIX paper

        (see the Lion's book for a MUCH better explanation)
        \paragraph{conceptual details}
        file - a string of bytes of any format. System does not care/enforce of
        their format.
        directory - an ordinary file (and can be read as such) but the system
        enforces a particular format (when writing)

        A file does NOT exist in a directory; it just exists and directories
        store a name + a pointer to it. It is usually deleted with the last
        link to it. It can still exist with no pointers to it (eg. unlinked
        everwhere, but someone still has it open)

        A dir has at least 2 entries:
            . - refers to the dir itself
            .. - referes to the parent in which it appears
        => except for these two, all dirs have exactly 1 parent and the FS is a
        rooted tree (this is why you can't hardlink dirs). Files have no such limit

        special files - behave exactly like normal files (can even link them)
        but writing them results in special behaviour (eg. /dev/sda writes the
        drive)

        mount - replaces a leaf (ordinary file) with the root of another FS.
        no links allowed between FSs (even .. refers to itself and not the
        parent on the root of the mount)

        permissions - 7 bits - 6 are the usual rwx, the 7th is setuid which
        changes the current user to the owner of the file \_only when executing
        it\_. In linux it's visible by an s instead of x on the permissions (eg
        -rwsr-xr-x for sudo).

        \paragraph{implementation details}

        directory - a list of (file, i-number (i=index)) pairs (14 and 2 bytes
        respectively, 16 B/entry, 32 entries per block) if i-number is 0, the
        entry is null. A regular file in all other regards

        i-list - a known location on list, which stores a physical pointer to
        an i-node. Its linearity allows a fast scan to check if free and used
        space sets are disjoint

        i-node - a structure that describes the file (owner, protection,
        physical address of contents, size, time of last modification, number
        of links to the file, directory bit, special file bit, "large"/"small" file
        bit). It's a 64 byte struct (Thompson 78)

        The i-number uniquely identifies a file, and open() translates from
        path to it

        drive is split into 512 byte blocks
        i-node - has room for 8 pointers to blocks (just a detail of UNIX structs)
        "large" (non-special) files - each pointer can point to an indirect
        block containing 256 addresses of actual data blocks (pointers were 2
        bytes on the PDP-11) (this gives a total size of 8*256*512 bytes (1M)
        for files)
        "small" (special) files - only the first pointer (2B) is used - 1B for
        device type (which driver), 1B for device number

        mount - keeps a dict with key: (i-number, device name of mount (path?),
        value: device name of the special file. On open(), the dict is searched
        and if a match is found, the i-number is replaced with 1 (the root
        directory is always 1) and device name becomes the value of the lookup

        block 0 - never used; It's NULL
        block 1 - the superblock

        struct filesys - the superblock.
            s\_ifree - a list of (up to) 100 free inode blocks. It is NOT
            exhaustive, but is rather a cache. When the list runs out, the
            whole FS has to be searched to refill it. If the list is
            overfilled, the new entries are forgotten, since they will go to
            disk anyway and can be found again when searching later.
            As far as I can tell, it's not strictly necessary

            s\_free - a linked list of free blocks.  Blocks are grouped in
            logical groups of 100 (may not be contiguous). Block 0 in each
            group is a table of the addresses of the next group of 100 blocks.
            The Head group's block 0 has a single entry of NULL to signify the
            end of the list. The addresses of the blocks in the tail group are
            stored in the superblock.

            When a block is needed, it is removed from the list in the
            superblock. When the last entry is read, the table in the 0th block
            is copied into the superblock (we advance the linked list). Reading
            NULL as a block address signifies list is empty (so no more free
            blocks).

            The struct is held in memory and regulary written to disk. No
            backups are made.

            NOTE: thompson78 has better explanations. He works with a later
            UNIX version though (I think it's v6, whereas the original is v5).
            Linked list is up to 50 entires, inodes are a block again, but have
            13 block pointers, first 10 are direct pointers, 10th is an
            indirect block (if necessary). 12 has double indirection, 13 has
            triple.
            McKusick says it has 4MB of inodes, 146MB of data for 150MB FS

    \subsection{FFS}
        McKusick 1984

        old filesystem - the second version above, with 1024B blocks (unsure if
        both data and metadata were affected)

        says UFS has very poor throughput. Seems to use the 13-entry inode
        version of UNIX. Same dir inodes aren't (usually) consecutive. File
        blocks frequently aren't either. Only used about 4\% of throughput,
        mostly because the free (linked) list became scrambled after use,
        making the blocks of new files to be random too

        blocks - 4KB (can be set higher at FS creation)
        fragment - a partition of a block (1 block = 2/4/8 fragments, lower
        bound is the sector size of the drive, usually 512B, configurable on FS
        creation)

        rotational positions - 8 counts of available blocks to easy cacluating
        timeouts for when a block will be availbe (so that block number
        requested can be as close to the head as possible). Uses rotational
        layout table

        superblock - replicated (and immutable, so restoring it from a copy is safe)

        free space - described by a bitmap (no longer a linked list). Each bit
        describes a \_fragment\_. Allocation can be on any fragment, although
        allocations of a full block must not cross block boundaries (must be
        aligned in a sense, eg. can't clump together 4 adjacent fragments on
        different blocks)

        inodes - allocates 1 for each 2048 bytes of space (expecting this to be
        more than needed)

        cylinder group - a number of consequtive cylinders on disk. Each one
        contains all the book keeping for it (inode list + free space bitmap +
        rotational positions) + a copy of the superblock. The superblock is
        offset by 1 track for each following cylinder group (so that it spirals
        down on platters and isn't all on the top one), the space in front is
        used for data blocks. (I presume the metadata is exactly after the
        superblock, but I can't be sure)

        allocation - each file has 0+ blocks and an optional fragmented block
        (which can be shared). When writing, the fragmented block is always
        kept for the end of the file and is copied forward (/expanded) to make
        sure. Whenever a number of fragments are needed, it first tries to use
        an already fragmented block. Thus wasted space is as if the FS had a
        block size of the fragment size, but performance is that of the block
        size. Drawback - FS can't be full (to maintain performance); a reserve
        is defined (10\% default) and only root can write after the reserve is
        reached (fs looks full for everyone else)

        allocation policy (when possible):
            - inodes of files in a directory are placed in the same cylinder
            - new dir is placed on a cylinder group with higher than avg free
              inodes and smallest number of dirs in it (intent: policy above to
              succeed more frequently)
            - next free for allocation of inodes within a group. This makes
              them random within it, but all inodes in a group (and hopefully a
              directory) can be read in 16 disk transfers at most (since a group
              can't have more than 2048 inodes)
            - big files - allocate a different cylinder group after 48KB and
              after 1MB thereafter. Group is chosen to have above avg free blocks
            - use a heuristic for finding a new block (escalates from same
              cylinder, same group, hash of group for new one, and then
              exhaustive search)

        performance - large blocks (fragments irrelevant) decrease write CPU
        utilization and increase read speed

        inodes - seems to be same layout (10 direct, 11 single indirect, 12
        double, 13 triple), but because of the larger block size, less
        indirection is required => less space used for large files
        (the bitmaps use extra, so FS has ~same bookkeeping size)

        directories - dirs allocated in chunks of 512B. An entry can't cross
        chunks. An entry has: i-node num, size of entry, length of filename,
        var len filename, paddings to 4 byte boundary (max filename is 255B
        though). Available space in a dir is added to the prev entry's size (so
        that it has more than expected bytes)

        \paragraph{"new" to UNIX}
        symbolic links - allowed to cross FS/machine. Is a file that contains
        the pathname from which to proceed
        rename syscall
        quotas

        supposedly this is in 4.3 BSD. Also supposedly it refuses to write over
        90\% (that's a quote from the paper if I remeber)
        Does lots of caching (says Beating the IO bottleneck).

    \subsection{LFS}
        \paragraph{Beating the IO bottleneck}
        First reference (?) to i-list being itself a file, scattered around
        disk, with only one superblock to describe it

        All files are stored sequetially on disk. The file data + header
        (inode) reside sequetially too. There's a "super-map" file which is a
        list of where we stored all inodes. Finally, the supermap is
        occasionally written to disk to commit the progress so far. With a bit
        of caching, read speed should be identical, writes much faster (when
        clustered together)

        Seems to be inspired by databases. Cites Hagmann as putting loggin in
        an FS first (but only metadata)

        \paragraph{The design and implementation of a (Sprite) LFS}

        FFS doesn't lay out metadata + data nearby. It requires 5 seeks to
        write a (small) file. It also does metadata writes synchronously, even
        though it does data async. That's slow for many small files

        inode - exact same as FFS. I.e. 10 direct bloks, 1 indirect, 1
        double-indrect, etc. (WAFL introduces the same lvl of indirectness for
        all blocks) (Once you have the inode, file access algorithm is the same
        in FFS and LFS)

        inode map - table containing the address of each inode. The map itself
        is a file, written to the log as such. There are fixed locations on the
        disk to store an address to it. Small enough to be cached and rarely
        read

        superblock - similar to FFS

        segment - a contiguous chunk of disk that is *always* written start to
        finish. It's size is such that transfer time of the segment is MUCH
        larger than the seek to it (they use 512K/1M)
        to limit segmentation it does 2 things:
          * copying - any data already on a segment has to be copied out of it
            before it is written
          * threading - live data is compacted into segments so that "full"
            segments can be skipped

        segment summary block - a block at the start of each segment that
        describes what's on it (eg inumber + block number). Can have multiple
        when a partial write has happened

        live block check - after determining which file the block in the
        segment belonged to, we can check the inode to see if it still points
        to it. Slight optimisation - keep a version number in of file, which
        can be used to compute a UID for every file (and store it in the block
        map and the segment summary; when they don't match, block is dead (and
        no need to read the inode)).

        segment cleaning - read n segments, identify the live data (using the
        stuff above), and write back fewer segments

        free list/bitmap - non existent. The segment/metadata allows us to
        check each one individually. Simplifies crash recovery

        policies:
        * when to clean - when fewer than n clean segments (n = arbitrary, 10s)
        * how much to clean - several tens
        * how long to clean for - until there are 50-100 clean ones
        * who to clean - see extensive chapter 3 on exactly that

        checkpoints - a fixed place which describes the last consistent place
        of the log

        roll-forward - the process of scanning the disk from the last
        checkpoint to the head of the log to recover as a much "lost" data as
        possible



        decribes a way to benchmarks file access patterns

        NOTE: modern FSs use a lite log. They log only the metadata


    \subsection{WAFL}
        4KB blocks, no fragments
        16 block ptrs:
            very small file - in the pointers
            up to 64K - direct ptrs
            up to 64M - single indirect and so on

        root inode - Has a whole block reserved for itself. Block is in a known
        (special) location. Points to inode file (itself)

        inode file - this is the i-list. Seems to work exactly as in UNIX,
        except it's a file itself (so gets scattered around as one, obeying the
        indirecion rules above). Contains:
            block file
            inode map file
            all other files

        block file - a map of 32 bits per block. Bit 0 set if active fs is referencing
        it, bit 1 for snapshot 1, etc

        inode map file - stores free inodes (format unknown, bitmap?)

        increasing FS size = increasing size of metadata files

        FS is structured as a tree, instead of linearly. This tree describes
        the FS tree on top of it

    \subsection{XFS}
        XFS paper

        Linear bitmaps of FFS aren't efficient for serching (esp when we need
        large contiguous blocks). FFS doesn't even try too hard to allocate
        contiguously

        Uses Allocation Groups (AG) (kind of like cylinder groups, but only for
        scalability, not disc locality) to split drive into 0.5/4 GB chunks.
        Also 2 B+ trees to store free space.

        "Extent based trees are more flexible and as efficient as bitmaps"

        Has sparse files. Stores files as a map of ranges where the files are

        Uses a B+ tree for literally everything

    \subsection{NTFS}

        Uses B-trees


    \subsection{ZFS}
        % some more info:
        % https://arstechnica.com/information-technology/2020/05/zfs-101-understanding-zfs-storage-and-performance/
        needs loads of cache for good read perf (stuff is scattered on disk) -
        60GB was given as a minimum work best when not filled (3/4 is max,
        recomemnded is no more that 50\% ful). Normal can go up to 90+\% I
        should probably focus on small systems (i.e. not datacentres)

        checksums - a blockchain. They are stored in the block above, meaning
        each checksum is itself checksumed from above. Only top one is stored
        with itself

        vdevs - a tree struct. Each vdev can have an arbitrary number of
        children, each of which can be either a vdev or a device. EG. 100GB
        mirror vdev: 2 children: 1 100GB disk vdev (with 100GB device) and 1
        100GB concat vdev (with 2 50GB device vdevs, each having 50GB devices)

        zpool - a self contianed pool of storage composed of any number of
        vdevs. Cannot share vdevs with one another. NO REDUNDANCY HERE. If one
        vdev dies, whole pool is out

        SPA - handles allocation of blocks (128 bit addresses)

        DMU - data management unit - MMU for drives. Sits on top of the SPA?

        Copy on write - editing a file causes a "ripple" of writes that ends in
        the overwriting of the uberblock. Until then, everything is consisten
        (the new one is not in the tree yet) and rewriting the uberblock is as
        atomic as possible

    \subsection{ext4}

        \paragraph{Inode table} Inode table is linear (not exactly dynamic (see
        free lists seciton for more), there's a linear formula to get block
        group number for an inode). Has a (linear) bitmap to track usage.  An
        inode is up to 256 bytes

        \paragraph{Free list} None (in a way). Disk is split into block groups,
        each group lists how many blocks/inodes in it are free. Both number of
        blocks and number of inodes is fixed. Scanning through all is somewhat
        faster


        % src: https://ext4.wiki.kernel.org/index.php/Ext4_Disk_Layout#Block_and_inode_Bitmaps
        \paragraph{File pointer tree} A node in the tree is followed (the node
        itself is a struct with a field on how many it has (and another one on
        how many it could have; presumably indicates available storage), and
        its last element is a var list). Length is arbitrary. Root node is in
        the inode (+ a few blocks). Only leaves point directly to data blocks.
        This removes a level of indirection. Balanced as a B-tree apparently


        \paragraph{Directories} (classic and not directly used) Basically a flat file
        with arbitrary filename strings. Organised in per-block sections, inode
        0 indicates unused (I think in practice a name is 255 or name\_max bytes)

        % described in the HTree paper; look at Linux src
        (modern and current) A Hashed Btree (Htree). Contains . and .. .
        Basically like a static BTree, keys indicate buckets of possible hashes
        (31 bit, last bit indicates a collision). A single node is a block
        long, limiting max number of keys in it. Has max 2 levels (although 1
        is common). Leaf nodes are the same layout as a classic dir (see above,
        although limited to 1 block in size) and contain entries that share the
        hash. If block is full, collision bit of keys is set and block is split
        (fixed op). Whole thing is layed out in a contiguous file and nodes
        start with zeores, so the whole directory file can be read in the old
        way, except it would appear as if it had more than expected empty
        entries.  Assuming a leaf holds 200 entries, 1 level is ~75K entries, 2
        lvl is ~30M.

        Hash function: used to be some weird hack hash, current one is TEA
        (Tiny Encription Algo). Both are about 20 LOC.

        Search in Htree keys: binary
        Search in Htree leaves: linear
        Performance: good, doesn't need rebalancing (unlike B/B*/B+ trees).
            Simple implementation. Hashes freqeuently used in other FSs


        NOTE: max filename lenth is 255 bytes on Linux (NAME\_MAX in linux/limits.h)

    \section{HFS+}
        \paragraph{Inode table} Called catalogue file. A Btree.
        \paragraph{Free list} A linear bitmap. Also just a file
        HFS+ has a field in the header saying which is the next free inode num

    \subsection{Soft updates}

        Part of UFS2, which is what freebsd uses. Has a quote (in intro) saying
        that a corrupt sector will (almost) always have an ECC which can detect
        partial writes => atomic single sector writes. Also most disks will not
        write a sector unless there is sufficient power to do so.
        => can result in loss of structure but not loss of integrity

        Requires no changes to on-disk layout. Main idea: keep a lot of
        information on every metadata update (most important infromation
        includes ordering constraints, i.e what need to be complete before this
        update can be requested so that FS stays consistent on disk). Then
        these writes are delayed (put in a buffer and left alone) to optimise
        writes on the same block (eg. creating many files in the same dir will
        prob have entries written to the same block and can be grouped). A
        background thread periodically runs (once a second) and flushes out
        some (any fraction) of the dirty blocks to disk. However, it only
        flushes updates that have had their constraints satisfied. If a dirty
        block has an update that still has pending requests, it is rolled back
        and not flushed (kept in the queue). After the the block is written,
        they can be reapplied to the dirty block in memory (and must be before
        it can accessed). For block deallocate/link delete, they are only freed
        after the pointer to be reset has been commited to disk

        Ordering constraints are put in to follow these rules:
          1 Never reset the old pointer to a resource before the new pointer has
            been set (when moving objects)
          2 Never re-use a resource before nullifying all previous pointers to
            it
          3 Never point to a structure before it has been initialized
        They mostly follow what would noramally be done synchronously, with the
        added benefit that disk state is left consistent. Still requires fsck
        to recover. Eg: normally, if the system crashes in the middle of moving
        a file, it would delete the link first and then create a new one,
        losing the pointer to the file. With this scheme the deletion will
        depend on the write and if it crashes in between there will be 2 links
        available. Still requires fsck but is simpler to recover from.

        They use metadata heavy benchmarks. 4 users copying a whole dir or
        deleting a load of files
    \subsection{Shadow pages, the postgres paper}

    \subsection{JFS which did journaling first}

        can't find an original source from IBM. Use the McKusick comparison
        with soft updates and Cedar File system reimplemntation.

    \subsection{Backblaze vault}
        \paragraph{Redundancy/Drive failure}

        A 17-3 data-paraity split (can be any ratio, this one is arbitrary).
        Essentially RAID, but FS is below the replication (i.e. it's flipped).
        Every disk has an ext4 FS and there's a SW stack on to to distribute
        files onto the 20 pieces (shards) (using Reed-Solomon coding which they
        open sourced ;)).

        Use ext4 because it handles:
        * power loss without losing files
        * lots of files with good access (on 1 drive)

        Comes out to 17.6\% overhead for parity, RAID6 on a 13-2 split is a
        15.4\% but doesn't have machine redundancy

        \paragraph{Data corruption}

        Every shard is checksumed, so errors are known. When discovered, can be
        recomputed because of the replication

        \paragraph{Data}
        They have LOTS of data
        \url{https://www.backblaze.com/b2/hard-drive-test-data.html#downloading-the-raw-hard-drive-test-data}
        for free.


\section{tools}
    \subsection{how to benchmark}

        \paragraph{Flexible IO tester (fio)} Can run "any" workload on a
        filesystem automatically. Useul for dev

        \paragraph{Postmark benchmark} A small file benchmark. Simulates a mail
        server.
        Citaiton seems to be: Jeffrey Katcher, PostMark: A New File System
        Benchmark. Link didn't load, this one is best so far
        https://www.yumpu.com/en/document/read/18367213/postmark-a-new-file-system-benchmark

    \subsection{LVM}
        Uses device mapper.

        ZFS paper: LVM can't be as good as the SPA because it loses all contextual
        info about blocks (it's blind). It still has to do contiguous allocations.
        Can't do sparse allocations

        me: , you can extend a logical volume (LV) with any amount of space (but
        prob just append the extra space to the volume). You can also shrink it (I
        presume you can remove one from the middle, if you tell it to move the
        contents, idk how the mapping is established)


\section{Humanitarian}
    \subsection{Failure oriented Computing}

        People always make mistakes, and they usually self-correct (i.e.
        mistake making is part of the human condition)

        margin of safety - to account for design, build, operation errors
        margin of ignorance - to account for what we don't know

        We need margins in computing

        Conduct error experiments with "Fault Ingection Glibc" (FIG)

        Even though RAID5 (1 failed drive) ought to be enough, lots of
        sysadmins have lost data regardless. Turns out, MTTF is not independent
        when drives are from the same batch.

    \subsection{RAID}
        Assumes independent and exponential failure rates.

        Boral 83 is a metric?

        \paragraph{notes from disperate reading}

        RAID kinda sucks because it doesn't address a corrupted sector (which
        can happen in an SSD, as far as I gather), and power loss: if it loses
        power during the write of the parity and the partiy is wrong, on
        reconsutrction the data will be wrong. This is essentially the same
        issue (?), but there is no way to know if the parity is correct.
        Reconstruction is a simple math operation, no checks happen

        "An Analysis of Data Corruption in the Storage Stack" - proves that
        silent erros occur lots. They even use a modified RAID that stores
        checksums itself (blocks are 520B, the last 8 are stored for a chunk of
        blocks on a separate block)

        HDDs do manage to detect and report lots of errors. But a lot also slip
        through. The paper above and the one by google for SSDs lead me to
        believe the situation is the same for SSDs (although more frequent).

        MDADM uses a journal to fix the write hole in RAID. I couldn't find
        anything to suggest any checksuming is doen to ensure data is not
        silently corrupt though (\url{https://lwn.net/Articles/665299/})


    \subsection{SSD failures}

        \paragraph{Google paper}
        At lest the drives Google use, have ECC in them. They seem to be able
        to detect all errors (even unrecoverabe ones). They distinguish between
        recoverable and unrecoverabe ones. Recoverable are plentiful,
        unrecoverabe less so. They don't seem to correlate, but unrecoverabe
        ones can cause data loss.

        They consider a chip failed, after it has more than 5\% bad blocks (or
        a temporal threshold for errors).

        Bad blocks/chips are remapped to spare ones. It's unclear to me how
        they avoid data loss in the process (they don't)

        SSDs need replacing less frequently than HDDs but encounter loads more
        (unrecoverabe) errors

        A majority of drives experience an unrecoverable error in the first 4
        years.

        \paragraph{Jeff Bonwick (zfs creator), 2D RAID}
        Drives (almost) don't wear out and fail suddenly.

        SSDs degrade over time. When new, 1 every 10\^-3/10\^-4 bits errors. When
        close to EOL, 1 in 10\^-2 bits are bad. Has 40 to 80 ECC for correction
        (BCH or LDPC codes). It even requires writes to be scrambled, because
        adjacent charges can interfere with each other (so we want data to be
        roughly random).

        Random vs sequential IO is the same, so queue depth doesn't have to be
        as deep (no need to sort and optimise).

        Introduces D-dimensional raid. Sectors/blocks are arranged in a D
        dimensional grid. When using parity of M, a failure in one row of M+1
        erros can be recovered from by using the other dimensions. He gave an
        example where he recovered from more than half the blocks having
        errors. Only "bad" failures are in a perfect square/cube/... where
        there error of M+1 is in all dimensions. I.e. (M+1)\^D in a perfect grid
        is the smallest unsolvable error.

        This only becomes space efficient with a large number of devices (SSD
        chips, which they have hunderds of in a drive, are suitable), at least
        for parity of 2 is overhead of 17\% (raid 6 pretty much). The fixed
        parity is fine, because flash is like CoW, and in place rewrites don't
        effectively happen. Higher capacity flash drives tend to last longer,
        because bandwidth tends to stay constant and increasing size gives more
        space to do wear leveling (and ssds fail due to writes).

        At VERY high frequencies, interrupts cause too many context switches =>
        overhead. Polling solves this, we onlly do the overhead when we need to

        NOTE: I'd need access to physical flash for this to be effective.

        Also an amazing visual for matrix multiply.

        "Flash is basically garbage. It barely works. We tolerate it because its fast"
        NOTE: I think the implication is that HDDs don't have ECC, while SSDs do.

\section{Observation}
    \subsection{evolution}

        It seems that UFS -> FFS -> LFS -> WAFL (maybe XFS?) -> ZFS is a
        natural evolution. Each one is a step up from the last.

        UFS introduces the idea.

        FFS makes it "fast" and considers inefficiencies, implementes policies
        and tries to keep things reasonably contiguous

        LFS invents the metadata as a file and starts to enfornce caching for
        good performance

        WAFL goes further and puts them all in a tree. Caching is a given at
        this point. It also changes the 10-1-1-1 ptrs with increasing lvls of
        indirectness, to having them all equally indirect

        ZFS pins the COW part, as WAFL mostly does it anyway. Caching is
        integral, otherwise zfs sucks

        Somewhere in there comes in XFS (or maybe the research one its based
        on) and introduces B-trees for efficient bookkeeping

    \subsection{The UFS name}

        UFS stands for Uniix File System, but not which precise implementation
        of it. It could be the original from 1974, could be FFS or could be
        UFS2 which can be any one of these.


\section{TODO}

    \subsection{Doing list}
        * QEMU instance
            - give it several hard drives and play around with LVM to see how
            it works, what it can/can't do. Specifically, allocate/partition
            the whole drive for standard install, then add a drive and extend
            both. NOTE: this can be done without QEMU, with loopback devices
        * try setting up a raid 1, flip a bit and see if FS corrects it

    \subsection{Reading list}
        * papers on FS testing (specifically Linux)

        Papers databases: https://library.leeds.ac.uk/subjects/1148/computing
        also: arxiv, library genesis
        meta database: https://www.re3data.org/ (has a pretty good pie chart to narrow down topics)

    \subsection{Things for the final report}
        Library has appointments for drafts, looking at it scoping docs, etc ("Help in person for taught students")
        \paragraph{Library suggestions}
        * Library has computing research database list; use that

        Things I want to consider to show **critical thingking**:
        all of this is in Critical thinking model, Paund and Elder, 2006
        * What theories/concepts are relevant?
          (where it came from, some of its definitions; i.e. give background on theories)
        * Identify assumptions and determine if they are justifiable
          (is it realistic to build a prototype for testing?, search for opposing
          theories/solutions; justify why you chose something and not another)
        * Consider implications/consequences, i.e. what may happend and what does
        * Consider other points of view
        * Why examine the issue at all?
        * What relevant data, xp are needed for assesment? Give evidence
        * What can be inferred from the evidence?
        * Consider making a concept map: a tree, root is the question, and then each level subdivides the topic
          eg: how much pollution are we exposed to when travelling -> air pollution -> CO, Pb, C6H6, NO2
                                                 -> transport    -> bus, cycle, on foot, train
        * Use proximity srch - when you expect words to be close; trunaction - for a root of a word
        These are advanced features of many search engines (google scholar isn't specialist; try specialist ones)
        and normal things like boolean logic, wildcards, you know the drill

        * look at how papers structure their intro: the last part is an outline of what will follow

        \paragraph{Ethics}
        email: r.morgan1@leeds.ac.uk for any help. Include project details, and
        lots of specifics and what you've got so far (look at outlook page for
        details on what they ask for)

        * I need to show I've considered ethics if I am to claim I have no
          ethical issues in my project

        * if collecting data: need free and informed consent (so need to know
          what data will be collected, what will be used for and not pressured
          into agreeing into it)

          if data identifies a person => personal data

          I am approved to collect some data, as long as follow some rules:
          * user testing - subject can leave at any time, concent form with full
            info on project and how data will be used, no sensitive information
            recorded/any personal info securely stored
          * public databases - is data legitimate, reliable, anonimized, does
            it respect privacy, concent from sources (either DB owner or subjects
            in DB)


\chapter{Thesis}
% How to format this: https://www.overleaf.com/learn/latex/How_to_Write_a_Thesis_in_LaTeX_(Part_1)%3A_Basic_Structure
    \section{NAME}
        \paragraph{A filesystem with native drive failure resiliance.}
        Very narrow topic, I won't consider any other aspects (unles trivial to
        do so)

    \section{Current plan}
        \paragraph{TODO} next meeting: aim for a concrete plan on how to implement that and
        test it. What workloads did they use? What metrics. Come up with important ones

        For the report: say how stupdendously difficult this is. I made a mistake
        of trying to make it linear, impelentation was stupid hard

        plan on how to demonstrate the quality of my report. Qualititavely and
        quantitavely

        DECLARE external code in report
        Rip out kernel code. Mention and discuss in report

        consider: propose purpose of fs to be a teaching FS, with all modern
        features. minimal Fs and say it could be taught to add redundancy as a
        case study (i.e. add it in a basic way and show it)

        Try to motivate why we need the FS.
        overhead mesurements. (turn off checksums and test again) measure costs
        of things

        success metrics: on par performance, correct silent errors

        Self-appraisal isn't a technical one. It's for the project. Any
        technical remarks should be in the main body of the report. Things like
        what i'd do differently, (if i had more time, knowledge etc) and ideas
        for future work

    \section{Feedback}
        This is a very interesting project well suited for a CS degree.

        Ensure your objectives are SMART, i.e. specific, measurable and timely.
        At present, they look like “features” to have. Once your objectives
        have been clearly identified you need a list of underlying tasks and
        milestones for the project.

        I assume your literature review is ongoing. Your project will require
        looking in detail at various papers in relation to the design and
        implementation of file system replication solutions.

        The Gantt chart  seems sensible.

        You have identified a number of risks. Ideally, present a risk
        mitigation table with the following information: risk identified,
        likelihood, impact, risk factor = likelihood x impact, and risk
        mitigation. Consider likelihood and impact in the range [very low, low,
        medium, high, very high].

        You should look at existing software/tools that you may end up
        using/integrating into your solution and their licenses. This is
        important for ethical/other issues.

        I look forward to seeing your demonstration and reading your final
        report.


    \seciton{meeting}

        * can I describe an FS and they NOT implement it fully? As in say i
          have btrees for free space, describe, analyse and everyhtin, but in
          self appraisal say i never got the time?

        * how to reference. As in, I need to describe the whole FS. Do i need
          to cite the paper for the whole page? Or just specific quotes?

\end{document}
